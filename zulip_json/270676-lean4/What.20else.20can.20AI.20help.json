[
    {
        "content": "<p>Hi everyone,<br>\n      I am an undergraduate student majoring in Artificial Intelligence, with a particular interest in Natural Language Processing (NLP). Recently, I have come across numerous research efforts that aim to automate theorem proving using neural provers and proof assistants like Lean. This has sparked my curiosity about AI4Math.<br>\n      Besides the NTP task, are there any other roles that AI can play in the development of formal mathematics, and what are the specific requirements that mathematicians have in this context?</p>",
        "id": 422961385,
        "sender_full_name": "Shaonan Wu",
        "timestamp": 1708672610
    },
    {
        "content": "<p>An even more science-fiction application is \"computer proves Riemann hypothesis\" but right now this looks a long way away (unless you're Christian Szegedy)</p>",
        "id": 422964949,
        "sender_full_name": "Kevin Buzzard",
        "timestamp": 1708674545
    },
    {
        "content": "<p>i know very little about NLP/FM crossover, but my impression is there's still a pretty huge gap in translating natural language descriptions of mathematical problems/structures/theorem statements into <em>accurate</em>/<em>correct</em> formal statements. this is entirely orthogonal to actually automating <em>proofs</em> once you have a formal statement, and yet seems to be very very hard in its own right.</p>\n<p>I think there is some interest in that direction from mathematicians, because having tools to translate natural language into formal language would make it easier to onboard more mathematicians to the formal world.</p>",
        "id": 422965891,
        "sender_full_name": "James Gallicchio",
        "timestamp": 1708674963
    },
    {
        "content": "<p>Given a mathematical claim, find counterexamples that disprove it. Currently there are a  few different methods of property testing. I don't know how many of those are integrated into lean</p>",
        "id": 422967014,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1708675508
    },
    {
        "content": "<p>Also see: <a href=\"https://youtu.be/vMLVH6IEwlM?si=dOfUFOlZMML3uuHU\">https://youtu.be/vMLVH6IEwlM?si=dOfUFOlZMML3uuHU</a></p>\n<div class=\"youtube-video message_inline_image\"><a data-id=\"vMLVH6IEwlM\" href=\"https://youtu.be/vMLVH6IEwlM?si=dOfUFOlZMML3uuHU\"><img src=\"https://uploads.zulipusercontent.net/879681f5538616c58e2f7757ec96d325ad1f132b/68747470733a2f2f692e7974696d672e636f6d2f76692f764d4c5648364945776c4d2f64656661756c742e6a7067\"></a></div>",
        "id": 422967032,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1708675526
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"407274\">James Gallicchio</span> <a href=\"#narrow/stream/270676-lean4/topic/What.20else.20can.20AI.20help/near/422965891\">said</a>:</p>\n<blockquote>\n<p>i know very little about NLP/FM crossover, but my impression is there's still a pretty huge gap in translating natural language descriptions of mathematical problems/structures/theorem statements into <em>accurate</em>/<em>correct</em> formal statements. this is entirely orthogonal to actually automating <em>proofs</em> once you have a formal statement, and yet seems to be very very hard in its own right.</p>\n<p>I think there is some interest in that direction from mathematicians, because having tools to translate natural language into formal language would make it easier to onboard more mathematicians to the formal world.</p>\n</blockquote>\n<p>I understand. Translating natural language into formal language is indeed similar to recent works in autoformalization, such as this paper(<a href=\"https://arxiv.org/abs/2210.12283\">https://arxiv.org/abs/2210.12283</a>). These works start with an informal proof and use it to generate a formal proof sketch through a language model, which is then refined using tools like Sledgehammer.</p>\n<p>Autoformalization is undoubtedly an important translation task. And how about the backtranslation task？？For example, given a less readable Lean code, generating math comments or translating it back into readable informal proofs can be valuable. This helps bridge the gap between formal and informal mathematical representations, making the formalized proofs more accessible and understandable to a wider audience.</p>",
        "id": 422972410,
        "sender_full_name": "Shaonan Wu",
        "timestamp": 1708678190
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"466334\">Shreyas Srinivas</span> <a href=\"#narrow/stream/270676-lean4/topic/What.20else.20can.20AI.20help/near/422967014\">said</a>:</p>\n<blockquote>\n<p>Given a mathematical claim, find counterexamples that disprove it. Currently there are a  few different methods of property testing. I don't know how many of those are integrated into lean</p>\n</blockquote>\n<p>Thank you for your reply. This is a truly new topic for me, and I will delve into it later.</p>",
        "id": 422972841,
        "sender_full_name": "Shaonan Wu",
        "timestamp": 1708678340
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"110038\">Kevin Buzzard</span> <a href=\"#narrow/stream/270676-lean4/topic/What.20else.20can.20AI.20help/near/422964949\">said</a>:</p>\n<blockquote>\n<p>An even more science-fiction application is \"computer proves Riemann hypothesis\" but right now this looks a long way away (unless you're Christian Szegedy)</p>\n</blockquote>\n<p>Yes, it is indeed challenging, and so far, it has mostly been depicted in fictional movies.</p>",
        "id": 422973098,
        "sender_full_name": "Shaonan Wu",
        "timestamp": 1708678448
    },
    {
        "content": "<p>Backtranslation is a lot easier, and GPT-4 can do it zero-shot rather well.</p>",
        "id": 422978527,
        "sender_full_name": "Siddhartha Gadgil",
        "timestamp": 1708680304
    },
    {
        "content": "<p>Regarding translation comments, I guess you could (reasonably quickly) develop some Encoder-Decoder Transformer architecture, provided enough examples of (informal, formal) pairs of theorems or lemmas are available. Otherwise, you might need to devise some intelligent system (maybe using existing LLMs in a few-shot manner) to curate a base dataset.</p>",
        "id": 423001946,
        "sender_full_name": "Gerard Calvo Bartra",
        "timestamp": 1708689467
    },
    {
        "content": "<p>Also, thinking outloud, the task of characterizing/ranking lemmas as \"useful\" or \"insignificant\" (and anything in between) is not trivial at all, and maybe one might use AI for this.</p>",
        "id": 423002576,
        "sender_full_name": "Gerard Calvo Bartra",
        "timestamp": 1708689741
    },
    {
        "content": "<p>There are plenty of examples of (informal, formal): the documentation strings of theorems and definitions in Mathlib. Including definitions this makes over 40000 examples.</p>",
        "id": 423004879,
        "sender_full_name": "Siddhartha Gadgil",
        "timestamp": 1708690696
    },
    {
        "content": "<p>Another task would be letting an AI create novel definitions of concepts based on TF-IDF (or similar) scores. For example, if it sees that some expression of the form <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>m</mi><mi mathvariant=\"normal\">∣</mi><mo stretchy=\"false\">(</mo><mi>a</mi><mo>−</mo><mi>b</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">m | (a-b)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\">m</span><span class=\"mord\">∣</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">a</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">−</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\">b</span><span class=\"mclose\">)</span></span></span></span> \"frequently\" appears in many theorems, then maybe it is worth defining <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>a</mi><mo>≡</mo><mi>b</mi></mrow><annotation encoding=\"application/x-tex\">a ≡ b</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4637em;\"></span><span class=\"mord mathnormal\">a</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">≡</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6944em;\"></span><span class=\"mord mathnormal\">b</span></span></span></span> (mod <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>m</mi></mrow><annotation encoding=\"application/x-tex\">m</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\">m</span></span></span></span>) if doing this can simplify anything.</p>",
        "id": 423005020,
        "sender_full_name": "Gerard Calvo Bartra",
        "timestamp": 1708690759
    },
    {
        "content": "<p>Conjecture generation is also an interesting topic.</p>",
        "id": 423005768,
        "sender_full_name": "Gerard Calvo Bartra",
        "timestamp": 1708691022
    },
    {
        "content": "<p>Possibly related to counterexample generation. Suppose I have a <code>theorem</code> statement in lean. Let's say I am not sure I have all the hypothesis I need. A useful AI could choose promising proof paths and produce a shortlist of hypothesis that it thinks you must add to your theorem statement to make it true. In a sense this can already be done using classical proof search methods. The AI application would be to filter out what might be the most promising pathways and suggest the corresponding hypothesis. A sort of truncated search.</p>",
        "id": 423104593,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1708727170
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"693186\">Gerard Calvo Bartra</span> <a href=\"#narrow/stream/270676-lean4/topic/What.20else.20can.20AI.20help/near/423002576\">said</a>:</p>\n<blockquote>\n<p>Also, thinking outloud, the task of characterizing/ranking lemmas as \"useful\" or \"insignificant\" (and anything in between) is not trivial at all, and maybe one might use AI for this.</p>\n</blockquote>\n<p>For what definition of useful? API lemmas are very useful because life is infinitely simpler when they exist. At the same time these are what mathematicians would consider trivial.</p>",
        "id": 423104715,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1708727234
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"693640\">@Shaonan Wu</span> Welcome.  If you haven't already, you should check out the stream: <a class=\"stream\" data-stream-id=\"219941\" href=\"/#narrow/stream/219941-Machine-Learning-for-Theorem-Proving\">#Machine Learning for Theorem Proving</a></p>",
        "id": 423126518,
        "sender_full_name": "Jason Rute",
        "timestamp": 1708744467
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"115715\">Jason Rute</span> <a href=\"#narrow/stream/270676-lean4/topic/What.20else.20can.20AI.20help/near/423126518\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"693640\">Shaonan Wu</span> Welcome.  If you haven't already, you should check out the stream: <a class=\"stream\" data-stream-id=\"219941\" href=\"/#narrow/stream/219941-Machine-Learning-for-Theorem-Proving\">#Machine Learning for Theorem Proving</a></p>\n</blockquote>\n<p>Yeah, I have explored some topics in this stream as well, and I must say that some of the works are truly impressive especially alphageometry, which showcases the power of synthetic data.</p>",
        "id": 423130263,
        "sender_full_name": "Shaonan Wu",
        "timestamp": 1708747444
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"466334\">Shreyas Srinivas</span> <a href=\"#narrow/stream/270676-lean4/topic/What.20else.20can.20AI.20help/near/423104593\">said</a>:</p>\n<blockquote>\n<p>Possibly related to counterexample generation. Suppose I have a <code>theorem</code> statement in lean. Let's say I am not sure I have all the hypothesis I need. A useful AI could choose promising proof paths and produce a shortlist of hypothesis that it thinks you must add to your theorem statement to make it true. In a sense this can already be done using classical proof search methods. The AI application would be to filter out what might be the most promising pathways and suggest the corresponding hypothesis. A sort of truncated search.</p>\n</blockquote>\n<p>I’m glad you brought this up. I’m particularly interested in this, too; it would be useful in science/engineering derivations. These often don’t specify all the premises needed, such as to avoid dividing by zero or requiring continuity and differentiability before taking a derivative). </p>\n<p>Here are the issues I anticipate such a “hypothesis generation” scheme will need to avoid:</p>\n<ol>\n<li>Generate a hypothesis “FALSE” and prove the conjecture from it. (Maybe obviously avoidable) </li>\n<li>Generate a hypothesis that contradicts an existing hypothesis. (More subtle than above, need good ways to detect and avoid contradictions among hypotheses)</li>\n<li>Generate the conjecture as a hypothesis. Prove with exact. (Maybe obviously avoidable)</li>\n<li>Generate something “too close for comfort” to the conjecture; it’s not the conjecture, but it’s close to it and renders the proof not useful.</li>\n<li>Generate hypotheses that narrow the scope too much.</li>\n</ol>\n<p>The typical use of AI to generate proofs is well-posed because the theorem statement is a static problem. But make the theorem statement fluid, and many varieties become possible; the space of these will include instances of high and low quality, and it will be likely much easier to generate low quality ones than high.</p>",
        "id": 423320148,
        "sender_full_name": "Tyler Josephson ⚛️",
        "timestamp": 1708920938
    },
    {
        "content": "<p>These issues are exactly why I think this is an AI problem. </p>\n<p>On the one hand it has some constraints which are somewhat well-defined but probably algorithmically hard to ensure. I.e. don't produce hypotheses that are identical to the conjecture, or worse, contradict other hypotheses. This means there has to be a provably correct algorithm that identifies whether a hypothesis does in fact contradict another. </p>\n<p>On the other hand there are ill-defined quality metrics which determine what makes a hypothesis high quality.</p>",
        "id": 423326923,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1708926191
    },
    {
        "content": "<p>From an algorithmist's perspective, I find AI great when:</p>\n<ol>\n<li>The problem objective is hard to define or ill-defined. This is the case of computer vision.</li>\n<li>The problem is computationally hard to solve as it is, but advice from an oracle of some accuracy (say an ML model) can greatly improve the computational complexity on useful inputs.</li>\n</ol>",
        "id": 423327213,
        "sender_full_name": "Shreyas Srinivas",
        "timestamp": 1708926423
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"466334\">@Shreyas Srinivas</span> <a href=\"https://news.ycombinator.com/item?id=39604600\">https://news.ycombinator.com/item?id=39604600</a> threw some classifiers at an elliptic curve property database (well, some L-function Dirichlet coefficients computed from them) to predict whether the curve had complex multiplication, and realized it performed better than expected.<br>\nThey investigated further with some related tasks and ended up plotting some information to realize that there was indeed a pattern clear enough to see, explaining how the classifiers could score so high.</p>\n<p>Finding <em>somehow</em> predictable, yet profoundly head-scratching patterns in discrete families of mathematical objects (AKA demonstrating statistically significant non-linear correlations) is something ML models are well-suited for.<br>\nUnexpected low perplexity is easy to spot and a clear sign of (unexpected) exploitable underlying structure.<br>\nAnnotating yet-unsolved samples with classification probabilities could further help guide effort (it's basically conjecturing when it makes high-confidence predictions).</p>",
        "id": 425441817,
        "sender_full_name": "namibj",
        "timestamp": 1709869799
    }
]