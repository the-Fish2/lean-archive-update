[
    {
        "content": "<p>It seems that most current approaches to ML proving are making the implicit assumption that it's better to work with tactics than raw terms. This isn't intuitively obvious to me, for a few reasons:</p>\n<ol>\n<li>The internal behaviour of tactics can be complex, and during training the model only sees the result of tactic applications, which is a fairly weak signal from which to infer that behaviour. In some sense, the tactics are actually hiding what's really happening - especially when they make use of implicit knowledge from the library.</li>\n<li>The relatively large number of tactics means that the model needs to learn a wide range of tactic behaviours, some of which are seen quite sparsely in the data. This may limit the ability of the model to generalise/share knowledge between proofs using different tactics.</li>\n<li>Training directly on terms is potentially more data efficient, because removing the abstraction means more data.  Every theorem would be an instance of the same problem, which potentially leads to better generalisation.</li>\n</ol>\n<p>Is there a good reason that tactic proofs are getting the focus at the moment, or is this one of those things that's just the current trend?</p>\n<p>My personal hunch is that tactic proofs are the easiest way to get to 35% of problems solved, because they automate so much of the proof process that the model can get away with not really \"understanding\" what it's doing. Many proofs are solvable by a sloppy application of common tactics. The success of the k-NN techniques show how far we can get with really dumb methods. However, if we want models to tackle the harder problems, I suspect we need to go lower level.</p>\n<p>Tactics have the advantage that they tend to produce nicer proofs for the user. If we care about that, we could always add a term proof -&gt; tactic proof translation step later on, so I don't think this should be considered as an important factor.</p>",
        "id": 423352759,
        "sender_full_name": "Sam",
        "timestamp": 1708938744
    },
    {
        "content": "<p>I should probably also mention that I'm currently playing around with some ideas (in the notes stage - I haven't even got to code yet) for an alternative way to represent term proofs to allow for a more iterative solving process.</p>\n<p>The basic idea is to take inspiration from e-graphs to represent partial term proofs, where a single proof step consists of non-destructively adding a new term to the e-graph. This would potentially allow iterative and out-of-order solving, and remove the need for backtracking in the proof search. I don't actually know if this idea works out yet, as it's still just a germ of a thought for now.</p>",
        "id": 423354753,
        "sender_full_name": "Sam",
        "timestamp": 1708939352
    },
    {
        "content": "<p>Tactics allow reasoning at a higher level of abstraction. This seems crucial to me. If you want to solve \"theorem proving\", then I think an AI might need to discover new abstractions on many levels:</p>\n<ul>\n<li>intermediate steps, <code>have</code>, forward reasoning, \"cut\"</li>\n<li>lemmas, theorems,</li>\n<li>definitions</li>\n<li>proof techniques, tactics (or some analogue/substitute)</li>\n</ul>\n<p>So I think it's fine to consider proof terms and train on those, but <em>I think</em> that avoiding tactics will not speed up the path to strong AI for theorem proving.</p>",
        "id": 423367809,
        "sender_full_name": "Johan Commelin",
        "timestamp": 1708943879
    },
    {
        "content": "<p>To what degree are the higher level of abstraction offered by tactics not just common patterns that reoccur in term proofs, which the model can learn to mimic?</p>\n<p>It's certainly true that tactics which internally do a search can make things easier for a model, because they reduce its burden. That's very helpful if you're doing a bruteforcy proof search with the model acting as a heuristic. However, if we want models which are good enough to put together working proofs without deep searches, it's less obvious to me that that's useful.</p>\n<p>For the model to be able to predict whether a tactic will work, it will usually need to be able to forsee what that tactic will do, and to know the outcome of the search. At that point, what purpose does the tactic serve?</p>",
        "id": 423370042,
        "sender_full_name": "Sam",
        "timestamp": 1708944629
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"428422\">@Sam</span>, you should look at the insanely large and near uninterpretable terms that even <code>simp</code> and <code>rw</code> produce, let alone <code>omega</code>, <code>ring</code>, <code>linarith</code>, etc.</p>",
        "id": 423374677,
        "sender_full_name": "Kim Morrison",
        "timestamp": 1708946371
    },
    {
        "content": "<p>It doesn't make sense to expect any model to synthesize these page long term mode proofs.</p>",
        "id": 423374725,
        "sender_full_name": "Kim Morrison",
        "timestamp": 1708946396
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"110087\">@Scott Morrison</span> Ah so perhaps this is a bad assumption on my part, but I assumed that in most cases those large term proofs are put together very inefficiently due to the rote way the tactics are implemented, and could usually be optimised to something much smaller. Is that not the case?</p>",
        "id": 423375089,
        "sender_full_name": "Sam",
        "timestamp": 1708946526
    },
    {
        "content": "<p>Not really. You can find some examples of <span class=\"user-mention\" data-user-id=\"470149\">@Joachim Breitner</span> making some very minor improvements to the terms produced by <code>rw</code> recently, but it just tinkering around the edges.</p>",
        "id": 423375790,
        "sender_full_name": "Kim Morrison",
        "timestamp": 1708946821
    },
    {
        "content": "<p>Tactics like <code>rw</code> have had a lot of effort making them efficient. But term size is not really a direct consideration for this!</p>",
        "id": 423376022,
        "sender_full_name": "Kim Morrison",
        "timestamp": 1708946867
    },
    {
        "content": "<p>I wasn't imagining optimising those tactics - I was imagining taking the term proofs they produce, and then running those through some kind of simplifier. But yeah - I don't actually know how viable this is!</p>",
        "id": 423376242,
        "sender_full_name": "Sam",
        "timestamp": 1708946944
    },
    {
        "content": "<p>There's not much to simplify. You need to write out a long sequence of congruence lemmas that allow you to modify a subexpression of a potentially large expression.</p>",
        "id": 423376368,
        "sender_full_name": "Kim Morrison",
        "timestamp": 1708946997
    },
    {
        "content": "<p>Ah, so that deep <code>rw</code> problem I maybe have a solution for... But probably not. (Disclaimer: I'm definitely the least educated person on these matters in this conversation.)</p>\n<p>My instinct is to try representing the partial term proofs as something e-graph like, meaning that deep sub-terms don't quite exist in that same way. A <code>rw</code> operation becomes a merging of two terms into the same equivalence class, but nothing needs to change \"upstream\". A proof is then said to be completed when there's an extractable term in the e-graph that gives the goal - but technically this can be concluded without even extracting the full term.</p>\n<p>But all of the above may be nonsense.</p>",
        "id": 423377264,
        "sender_full_name": "Sam",
        "timestamp": 1708947368
    },
    {
        "content": "<p>I'm not 100% sure that actually addresses the problem you're referring to!</p>",
        "id": 423377921,
        "sender_full_name": "Sam",
        "timestamp": 1708947599
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"110087\">Scott Morrison</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/ML.20on.20tactics.20vs.20terms.3F/near/423376368\">said</a>:</p>\n<blockquote>\n<p>There's not much to simplify. You need to write out a long sequence of congruence lemmas that allow you to modify a subexpression of a potentially large expression.</p>\n</blockquote>\n<p>No, the proofs generated by most lean tactics are massively inefficient compared to what a human would do</p>",
        "id": 423389623,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1708952099
    },
    {
        "content": "<p>it's just (1) difficult to find shorter proofs without taking longer, and (2) less uniform and more complex for the tactic code</p>",
        "id": 423389752,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1708952143
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"110049\">@Mario Carneiro</span> Do you have a sense of how practical it would be to optimise those generated proofs using rewrite rules, simp style?</p>",
        "id": 423389998,
        "sender_full_name": "Sam",
        "timestamp": 1708952216
    },
    {
        "content": "<p>It won't work because <code>simp</code> explicitly disables itself on proofs</p>",
        "id": 423390092,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1708952249
    },
    {
        "content": "<p>but assuming it was actually treated as a goal, I think you could get quite a bit closer to hand-optimized proofs that way</p>",
        "id": 423390270,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1708952298
    },
    {
        "content": "<p>Hmm, that <code>simp</code> issue seems like the type of thing that would probably be workaroundable, right? Like if we just pull the theorem values as <code>Expr</code>s it should be possible to run <code>simp</code> on those? Or maybe there's some additional complexity here I'm not aware of.</p>",
        "id": 423390493,
        "sender_full_name": "Sam",
        "timestamp": 1708952388
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"428422\">@Sam</span> Or you can do both.  :)  <a href=\"https://arxiv.org/abs/2102.06203\">PACT</a> added the term proofs to the training data in two forms, both <code>exact &lt;whole proof&gt;</code> and <code>apply &lt;next step</code>&gt; for every subtree in the full proof.  This data (along with other data we added) seems to really help the results.  The <a href=\"https://arxiv.org/search/cs?searchtype=author&amp;query=Polu,+S\">other Lean papers by OpenAI</a> as well as the <a href=\"https://arxiv.org/abs/2205.11491\">HTPS paper</a> by Meta all used this data.  And you could see it in the results (when these models used to be publically available for Lean users to play with).  Often the model predicted <code>exact &lt;some long term proof&gt;</code> and it was correct.</p>",
        "id": 423421402,
        "sender_full_name": "Jason Rute",
        "timestamp": 1708961432
    },
    {
        "content": "<p>I gave a talk on PACT in this youtube video: <a href=\"https://www.youtube.com/watch?v=EXpmbAfBNnw\">https://www.youtube.com/watch?v=EXpmbAfBNnw</a></p>\n<div class=\"youtube-video message_inline_image\"><a data-id=\"EXpmbAfBNnw\" href=\"https://www.youtube.com/watch?v=EXpmbAfBNnw\"><img src=\"https://uploads.zulipusercontent.net/bd2a9c6a819c867efb19ea439f4a113b891f5dae/68747470733a2f2f692e7974696d672e636f6d2f76692f4558706d624166424e6e772f64656661756c742e6a7067\"></a></div>",
        "id": 423421724,
        "sender_full_name": "Jason Rute",
        "timestamp": 1708961503
    },
    {
        "content": "<p>It is sort of funny that many recent tools like Lean Copilot don't use data like this.  It is pretty easy to gather and it seems to help the performance.</p>",
        "id": 423422155,
        "sender_full_name": "Jason Rute",
        "timestamp": 1708961612
    },
    {
        "content": "<p>Oh, and here is another recent paper which reimplemented PACT: <a href=\"https://aclanthology.org/2023.acl-long.706.pdf\">DT-Solver</a></p>",
        "id": 423422524,
        "sender_full_name": "Jason Rute",
        "timestamp": 1708961706
    },
    {
        "content": "<p>Agda is interesting in that it doesn't have tactics (I think).  So it likely has better term-proof data.  Here is an Agda dataset, but honestly I haven't looked into it closer: <a href=\"https://arxiv.org/pdf/2402.02104.pdf\">https://arxiv.org/pdf/2402.02104.pdf</a></p>",
        "id": 423423181,
        "sender_full_name": "Jason Rute",
        "timestamp": 1708961913
    },
    {
        "content": "<p>(I should probably make a new thread for that paper.)</p>",
        "id": 423423244,
        "sender_full_name": "Jason Rute",
        "timestamp": 1708961933
    },
    {
        "content": "<p>Ah, thanks for pointing out PACT <span class=\"user-mention\" data-user-id=\"115715\">@Jason Rute</span>! I'm actually just realising that I watched that talk back when it came out, but 2021 is ancient history so it slipped my mind. I don't know if I ever fully read the paper though - so that's on my reading list now!</p>",
        "id": 423424110,
        "sender_full_name": "Sam",
        "timestamp": 1708962187
    },
    {
        "content": "<p>Finally, I sometimes wonder if we should work more on AI for lower-level proof languages like Metamath and term proofs.  The reason is not that we will solve more theorems without tactics.  Tactics clearly help, ... a lot.  (Just look at <a href=\"https://paperswithcode.com/sota/automated-theorem-proving-on-minif2f-test\">minif2f results for GPT-f in Metamath</a>.)  But because our current approaches rarely generate proofs that are more than say 3-5 tactics long it seems that we need new techniques and the utter failure on non-tactic benchmarks is a good place to start maybe.</p>",
        "id": 423424212,
        "sender_full_name": "Jason Rute",
        "timestamp": 1708962223
    },
    {
        "content": "<p>Also, to add to my list of results above, I should mention <a href=\"http://aitp-conference.org/2022/abstract/AITP_2022_paper_5.pdf\">this short project on generating synthetic term proofs</a> in Lean.</p>",
        "id": 423425195,
        "sender_full_name": "Jason Rute",
        "timestamp": 1708962484
    },
    {
        "content": "<p>Yeah your recent Graph2Tac work seems to be in the direction which I've always assumed was promising, so that's very cool! I've been wondering whether a pure term approach makes more sense when you've got a graph of the library as input. If we treat the proof process as being (approximately) a pathfinding task, then operating directly on low level terms seems in many ways simpler. It (almost) becomes a subgraph extraction problem, + synthesised terms.</p>",
        "id": 423425585,
        "sender_full_name": "Sam",
        "timestamp": 1708962581
    },
    {
        "content": "<p>I (unreasonably) view tactics as cheating. It seems to me that they allow the model to shortcut actually solving the problem, because the model just \"guesses\" that a certain powerful tactic will find <em>a</em> solution, but doesn't know <em>what</em> solution it will find.</p>",
        "id": 423426187,
        "sender_full_name": "Sam",
        "timestamp": 1708962760
    },
    {
        "content": "<p>Doesn't that mean it's incapable of scaling to larger proofs at some point? When there's a trial-and-error process involved, there's a deep search, and it's very easy to hit combinatorial explosion.</p>",
        "id": 423426614,
        "sender_full_name": "Sam",
        "timestamp": 1708962890
    },
    {
        "content": "<p>My guess would be that, for sufficiently complex tactics, the model will never be able to reliably predict whether a tactic is correct in a given state. <em>If it were good enough</em>, it would necessarily have modeled the tactic's behaviour so well that the tactic itself is redundant because the model knows what it will do.</p>",
        "id": 423427330,
        "sender_full_name": "Sam",
        "timestamp": 1708963086
    },
    {
        "content": "<p>Or I'm wrong, and the tactic approach heuristic can get <em>good enough</em> to keep searches reasonably bounded even for large proofs. <span aria-label=\"shrug\" class=\"emoji emoji-1f937\" role=\"img\" title=\"shrug\">:shrug:</span></p>",
        "id": 423428852,
        "sender_full_name": "Sam",
        "timestamp": 1708963522
    },
    {
        "content": "<p>Suppose I have an informal representations of things, say, natural language, then terms in that informal representations can be seen as tactics in Lean. In this sense, tactics actually should be the output target. It's too hard to directly output a detailed low-level proof. It's sensible to output first a high-level proof sketch that is correct with high probability, then lowering it to a low-level rigorous proof. This is what mathematicians do in their brain.</p>",
        "id": 423435141,
        "sender_full_name": "Xiyu Zhai",
        "timestamp": 1708965140
    },
    {
        "content": "<p>To be clear, I'm not advocating generating whole term proofs dirctly with an autoregressive model. I'd assume some type of hierarchical decomposition of terms which can be generated out of order, so there's still some ability to do high level -&gt; low level reasoning, preferably using a learned strategy.</p>",
        "id": 423437488,
        "sender_full_name": "Sam",
        "timestamp": 1708965680
    },
    {
        "content": "<p>I would also say that, to some degree, a high level proof sketch can exist in latent space.</p>",
        "id": 423437861,
        "sender_full_name": "Sam",
        "timestamp": 1708965741
    },
    {
        "content": "<p>I would say the whole thing is very exciting. A lot of room for breakthroughs.</p>",
        "id": 423442404,
        "sender_full_name": "Xiyu Zhai",
        "timestamp": 1708966813
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"428422\">Sam</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/ML.20on.20tactics.20vs.20terms.3F/near/423437861\">ha dicho</a>:</p>\n<blockquote>\n<p>I would also say that, to some degree, a high level proof sketch can exist in latent space.</p>\n</blockquote>\n<p>This is very interesting. Maybe there is some way to force the latent space to form some \"distribution\" that structures it as high-level proof sketches, similar to what VAE models do, but surely not by forcing a Gaussian distribution. Does anyone have any idea what it would look like? I imagine it would be like a graph or tree sketch... This might be achievable by playing with the latent space and the loss or regularization term.</p>",
        "id": 423568532,
        "sender_full_name": "Gerard Calvo Bartra",
        "timestamp": 1709027220
    },
    {
        "content": "<p>Maybe forcing the latent space to be represented by graphs is too restrictive, but similar ideas could be used to force the latent space to follow certain valuable properties in a more \"loose\" way. For instance, Topological Data Analysis (TDA) could be used to encourage the latent space to organize itself in a way that preserves the high-level structure of the proof space, right? (I'm no expert in TDA, but it intuitively sounds good to me).</p>",
        "id": 423593792,
        "sender_full_name": "Gerard Calvo Bartra",
        "timestamp": 1709036244
    },
    {
        "content": "<p>As for latent representations, I had some ideas in that direction here: <a href=\"https://github.com/jasonrute/thoughts-on-ai-for-theorem-proving/blob/main/search.md\">https://github.com/jasonrute/thoughts-on-ai-for-theorem-proving/blob/main/search.md</a>  (Most of those ideas I’ve never had a chance to work on.)</p>",
        "id": 423595535,
        "sender_full_name": "Jason Rute",
        "timestamp": 1709036981
    }
]