[
    {
        "content": "<p>This paper just came on my feed.  <a href=\"https://arxiv.org/abs/2312.14188\">https://arxiv.org/abs/2312.14188</a>   Looks like another neural theorem prover for Lean.  I have looked at the details yet.</p>",
        "id": 410282494,
        "sender_full_name": "Jason Rute",
        "timestamp": 1703763805
    },
    {
        "content": "<p>Maybe the authors <span class=\"user-mention\" data-user-id=\"625933\">@Rahul Vishwakarma</span> and Subhankar Mishra would like to say more about it.</p>",
        "id": 410282693,
        "sender_full_name": "Jason Rute",
        "timestamp": 1703763911
    },
    {
        "content": "<p>The model seems to be called DS-Prover.</p>",
        "id": 410283380,
        "sender_full_name": "Jason Rute",
        "timestamp": 1703764296
    },
    {
        "content": "<p>Is the code for splitting up tactic steps into multiple steps available somewhere?</p>",
        "id": 410304532,
        "sender_full_name": "Adam Topaz",
        "timestamp": 1703775281
    },
    {
        "content": "<p>BTW, <code>simp [p1, ..., pn]</code> and <code>simp [p1]; ...; simp [pn]</code> are generally not equivalent, but maybe that's not too much of an issue for the purposes of getting training data.</p>",
        "id": 410304585,
        "sender_full_name": "Adam Topaz",
        "timestamp": 1703775308
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"243562\">@Adam Topaz</span> are you referring to something in the paper?</p>",
        "id": 410305581,
        "sender_full_name": "Jason Rute",
        "timestamp": 1703775775
    },
    {
        "content": "<p>Yes</p>",
        "id": 410305847,
        "sender_full_name": "Adam Topaz",
        "timestamp": 1703775879
    },
    {
        "content": "<p>Oh I see, 3.2.2.  Yes, that simp example seems problematic.  It would be better to do this with metaprogramming to get things right.  Some of the Coq papers split tactics like this with metaprogramming.</p>",
        "id": 410305881,
        "sender_full_name": "Jason Rute",
        "timestamp": 1703775893
    },
    {
        "content": "<p>The appendix A.1 has a table of all the conversions</p>",
        "id": 410305944,
        "sender_full_name": "Adam Topaz",
        "timestamp": 1703775932
    },
    {
        "content": "<p><a href=\"/user_uploads/3121/5Fbq9Ue1R1GWFQ7lF-ZBM3vU/image.png\">image.png</a><br>\n!!!</p>\n<div class=\"message_inline_image\"><a href=\"/user_uploads/3121/5Fbq9Ue1R1GWFQ7lF-ZBM3vU/image.png\" title=\"image.png\"><img src=\"/user_uploads/3121/5Fbq9Ue1R1GWFQ7lF-ZBM3vU/image.png\"></a></div>",
        "id": 410306772,
        "sender_full_name": "Eric Rodriguez",
        "timestamp": 1703776299
    },
    {
        "content": "<p>In this paper, we enhance the neural theorem prover using data augmentation and a dynamic sampling method.</p>\n<p><strong>Data Augmentation:</strong> ML model learns to predict the tactics from the training data. To enhance the training data, we increased the number of tactic goal pairs by creating new tactic goal pairs by decomposing the rewrite and simplification tactics with multiple premises into tactics with single premises e.g. changing ‘rw [p1, p2]’ into ‘rw [p1]’ and ‘rw [p2]’. This provides the model with a larger number of examples to learn from and helps it handle the diverse goals better while generating tactics.  </p>\n<p><strong>Dynamic Sampling</strong> (DS-Prover) is a small improvement over fixed sampling (sampling same number of tactics each time to expand the goal) method where we decide the number of tactics to sample and apply to expand the current goal, taking into account the remaining time compared to the total allocated time for proving a theorem. </p>\n<p>In the fixed sampling method, the number of nodes increases exponentially (at a depth) as the search tree depth increases. This makes this method harder to explore the proofs with a larger number of proofsteps (higher tree depth).</p>\n<p>In dynamic sampling, we start with a larger number (n) of tactics to sample initially and then decrease n as time passes. This method improves the prover's performance by countering the exponential growth of the number of nodes at the increasing depth of the search tree, making the prover better at exploring the proofs even with larger number of proof steps along with smaller ones.</p>",
        "id": 410312732,
        "sender_full_name": "Rahul Vishwakarma",
        "timestamp": 1703779150
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"243562\">Adam Topaz</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/Paper.3A.20Enhancing.20Neural.20Theorem.20Proving.20through.20Data.20Augm.2E.2E.2E/near/410304585\">said</a>:</p>\n<blockquote>\n<p>BTW, <code>simp [p1, ..., pn]</code> and <code>simp [p1]; ...; simp [pn]</code> are generally not equivalent, but maybe that's not too much of an issue for the purposes of getting training data.</p>\n</blockquote>\n<p>Hmm, we cannot replace them directly. So, we had to look at all the possibilities. We have explained this in detail in section 3.2.2 (Data Collection) of the paper.</p>",
        "id": 410313850,
        "sender_full_name": "Rahul Vishwakarma",
        "timestamp": 1703779572
    },
    {
        "content": "<p>Thanks for the clarification!</p>",
        "id": 410314183,
        "sender_full_name": "Adam Topaz",
        "timestamp": 1703779715
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"115715\">Jason Rute</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/Paper.3A.20Enhancing.20Neural.20Theorem.20Proving.20through.20Data.20Augm.2E.2E.2E/near/410305881\">said</a>:</p>\n<blockquote>\n<p>Oh I see, 3.2.2.  Yes, that simp example seems problematic.  It would be better to do this with metaprogramming to get things right.  Some of the Coq papers split tactics like this with metaprogramming.</p>\n</blockquote>\n<p><strong>Here is the description of how we collected this data:</strong> <br>\nSince the list of premises in the simplification (simp, dsimp, simpa, simp rw) tactics is not ordered i.e. the premises can be used in any order to simplify the goal. So, to collect the goals corresponding to each new simplification tactics we used LeanDojo to apply all the new tactics one by one at the goal of the step where the original simplification tactic was supposed to be applied and record the tactic goal pair whichever succeeds and keep on repeating this task on the subgoals generated by the application of first successful tactic and we don’t keep the successful tactics in the future list.</p>",
        "id": 410314639,
        "sender_full_name": "Rahul Vishwakarma",
        "timestamp": 1703779944
    },
    {
        "content": "<p>The <code>simpa</code> example there ought to fail no matter what if this is done; I think the only way you could approximate simpa with a chain of tactics is turning <code>simpa [p1, p2,...] using &lt;a&gt;</code> into <code>have := &lt;a&gt;; simp [p1] at this |-;... simpa [pn] using this</code>; is there any successful <code>simpa</code> calls in the generated dataset?</p>",
        "id": 410319687,
        "sender_full_name": "Eric Rodriguez",
        "timestamp": 1703782826
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"284160\">Eric Rodriguez</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/Paper.3A.20Enhancing.20Neural.20Theorem.20Proving.20through.20Data.20Augm.2E.2E.2E/near/410319687\">said</a>:</p>\n<blockquote>\n<p>The <code>simpa</code> example there ought to fail no matter what if this is done; I think the only way you could approximate simpa with a chain of tactics is turning <code>simpa [p1, p2,...] using &lt;a&gt;</code> into <code>have := &lt;a&gt;; simp [p1] at this |-;... simpa [pn] using this</code>; is there any successful <code>simpa</code> calls in the generated dataset?</p>\n</blockquote>\n<p>Thanks for pointing this out.<br>\nI wasn't aware of this property of the 'simpa' tactic, so I applied the same method to all the simplification tactics.<br>\nRegarding the presence of 'simpa' in the generated data, I will check it and provide an update here.</p>",
        "id": 410322289,
        "sender_full_name": "Rahul Vishwakarma",
        "timestamp": 1703784413
    },
    {
        "content": "<p>I’m happy to see papers looking more into search algorithms.  I think we are currently too focused on the prediction algorithms and not how to use them most efficiently to search.</p>",
        "id": 410324315,
        "sender_full_name": "Jason Rute",
        "timestamp": 1703785705
    },
    {
        "content": "<p>Proverbot9001 runs into a similar issue when decomposing tactics for data augmentation, because many compound tactics have non-trivial semantics that can't be fully decomposed. But the way people use them most of the time doesn't run into those cases. So what we do to solve that is that our decomposition procedure (in proverbot terminology, the \"lineaerizer\") is running the decomposed proofs as it produces them, both because it needs some dynamic information to do the decomposition (<code>;</code> in coq is complicated), and because that means it can check the decomposed proofs at the end. Then, if decomposition fails on a particular proof, we can just return the original proof and move on. So it's a \"best-effort\" decomposition that works in 95% of cases, and is always sound. </p>\n<p>So if you want to keep your ability to decompose more tactics, you might want to try that sort of \"check and fallback\" strategy. You can even do it at a much finer granularity than full proofs with some effort, just keep track of what each tactic produced in it's original form and make sure it produces an identical state in it's decomposed form. That way, if one <code>simpa</code> can't decompose, you can keep that one compound and still decompose the rest of the proof.</p>",
        "id": 410345206,
        "sender_full_name": "Alex Sanchez-Stern",
        "timestamp": 1703799980
    },
    {
        "content": "<p>Also, do you have any evidence that allowing more predictions at the beginning and fewer predictions in the end is better than allowing fewer predictions in the beginning, and then more at the end? Both combat the exponential growth of the search tree the same, but it is often more \"obvious\" what the correct step is during the initial steps in a proof than deep in a proof, from a human perspective.</p>",
        "id": 410345717,
        "sender_full_name": "Alex Sanchez-Stern",
        "timestamp": 1703800384
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"659851\">Alex Sanchez-Stern</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/Paper.3A.20Enhancing.20Neural.20Theorem.20Proving.20through.20Data.20Augm.2E.2E.2E/near/410345717\">said</a>:</p>\n<blockquote>\n<p>Also, do you have any evidence that allowing more predictions at the beginning and fewer predictions in the end is better than allowing fewer predictions in the beginning, and then more at the end? Both combat the exponential growth of the search tree the same, but it is often more \"obvious\" what the correct step is during the initial steps in a proof than deep in a proof, from a human perspective.</p>\n</blockquote>\n<p><strong>We chose a decreasing (high to low) number of tactic sampling method because of the following reasons:</strong></p>\n<ol>\n<li>\n<p>The search algorithm often encounters situations where the queue, containing the nodes to explore, becomes empty. This occurs when the number of new nodes being added to the queue is fewer than the nodes being explored and removed, leading to an early stoppage without any proof. To prevent such cases, we decided to start with a larger number of tactics to sample. This approach enables us to add a greater number of nodes to the queue initially, thereby maintaining an adequate number of nodes throughout the proof search.</p>\n</li>\n<li>\n<p>Upon analyzing the proofs of the theorems in mathlib, we discovered that there exists a larger number of theorems with smaller proof sizes, and vice versa. As the tree depth increases, the number of paths leading to that depth also exponentially increases, providing more options for finding the proof. Conversely, for shorter proofs, this is not the case. Hence, we need to initially sample a higher number of tactics for the prover to have enough options to find shorter proofs.</p>\n</li>\n<li>\n<p>Another reason why we thought of this idea is based on exploration and exploitation. Initially (when we start the proof search) we have enough time to explore and hence we even look at the less probable tactics generated from the model, whereas as time passes we try to save time (for exploring higher depth) by focusing only on the tactics which are more probable (has higher chances of success).</p>\n</li>\n</ol>",
        "id": 410410396,
        "sender_full_name": "Rahul Vishwakarma",
        "timestamp": 1703842338
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"284160\">Eric Rodriguez</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/Paper.3A.20Enhancing.20Neural.20Theorem.20Proving.20through.20Data.20Augm.2E.2E.2E/near/410319687\">said</a>:</p>\n<blockquote>\n<p>is there any successful <code>simpa</code> calls in the generated dataset?</p>\n</blockquote>\n<p>Upon analyzing the data, I found that only 102 out of 7384 total decomposed 'simpa' tactics were able to reduce their current goal into subgoals, and only those 102 tactic-goal pairs are present in the augmented dataset. Since we are only adding the successful tactic-goal pairs to the augmented data and discarding the unsuccessful ones, I don't think this method will cause any issues.</p>",
        "id": 410422663,
        "sender_full_name": "Rahul Vishwakarma",
        "timestamp": 1703849401
    }
]