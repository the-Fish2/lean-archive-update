[
    {
        "content": "<p>We released a series of math LLM named InternLM-Math-Plus, of sizes 1.8B, 7B, 20B, and 8x22B. Our models are equipped with COT reasoning, code-interpreter reasoning, and LEAN 4 abilities (43.4% pass@1 on MiniF2F-test set).</p>\n<p>Demo: <a href=\"https://huggingface.co/spaces/internlm/internlm2-math-7b\">https://huggingface.co/spaces/internlm/internlm2-math-7b</a></p>\n<p>Model: <a href=\"https://huggingface.co/internlm/internlm2-math-plus-mixtral8x22b\">https://huggingface.co/internlm/internlm2-math-plus-mixtral8x22b</a></p>\n<p>Code: <a href=\"https://github.com/InternLM/InternLM-Math\">https://github.com/InternLM/InternLM-Math</a></p>\n<p>Arxiv: <a href=\"https://arxiv.org/abs/2402.06332\">https://arxiv.org/abs/2402.06332</a></p>",
        "id": 440957555,
        "sender_full_name": "Zijian Wu",
        "timestamp": 1716867011
    },
    {
        "content": "<p>Very good job! I would like to ask if pass@1 here refers to tree search once or model generation once?</p>",
        "id": 440960636,
        "sender_full_name": "Huajian Xin",
        "timestamp": 1716868961
    },
    {
        "content": "<p>It is tree search once (like HyperTree Proof Search's definition). In addition, the tree search width is 32, following the setting of llemma.</p>",
        "id": 440960941,
        "sender_full_name": "Zijian Wu",
        "timestamp": 1716869245
    }
]