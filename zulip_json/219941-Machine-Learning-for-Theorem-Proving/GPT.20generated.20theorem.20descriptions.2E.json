[
    {
        "content": "<p>Continuing from <a href=\"#narrow/stream/113486-announce/topic/GPT-4.20generated.20theorem.20descriptions/near/436189082\">https://leanprover.zulipchat.com/#narrow/stream/113486-announce/topic/GPT-4.20generated.20theorem.20descriptions/near/436189082</a></p>",
        "id": 436203250,
        "sender_full_name": "Kim Morrison",
        "timestamp": 1714458531
    },
    {
        "content": "<p>I'm inclined to think these are much too verbose.</p>\n<blockquote>\n<p><code>theorem Nat.Prime.one_lt : ∀ {p : ℕ}, Nat.Prime p → 1 &lt; p</code><br>\nThe theorem <code>Nat.Prime.one_lt</code> states that for all natural numbers <code>p</code>, if <code>p</code> is a prime number, then <code>p</code> is greater than 1. In other words, if a natural number is prime, it must be strictly larger than 1.</p>\n</blockquote>\n<p>would be much better as just \"if a natural number is prime, it must be strictly larger than 1\"</p>",
        "id": 436203384,
        "sender_full_name": "Kim Morrison",
        "timestamp": 1714458604
    },
    {
        "content": "<p>So we can now document all mathlib lemmas right? :-)</p>",
        "id": 436203506,
        "sender_full_name": "Kevin Buzzard",
        "timestamp": 1714458656
    },
    {
        "content": "<p>I'd also like to see some automatic way of quantifying the quality of auto-informalization. I think a reasonable one is to atuo-formalize the resulting text, and see if it is identical (equivalent up to aesop??) to the original.</p>",
        "id": 436203860,
        "sender_full_name": "Kim Morrison",
        "timestamp": 1714458828
    },
    {
        "content": "<p>This would be more likely to work the more verbose the translation, right?</p>\n<p>I think that formalising these informal statements sounds like a great idea! If we can get machines translating latex to lean then I can get FLT done much quicker. Right now I guess it's science fiction but practicing on theorem statements is surely a good place to start</p>",
        "id": 436206914,
        "sender_full_name": "Kevin Buzzard",
        "timestamp": 1714460141
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"110038\">Kevin Buzzard</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/GPT.20generated.20theorem.20descriptions.2E/near/436206914\">said</a>:</p>\n<blockquote>\n<p>This would be more likely to work the more verbose the translation, right?</p>\n<p>I think that formalising these informal statements sounds like a great idea! If we can get machines translating latex to lean then I can get FLT done much quicker. Right now I guess it's science fiction but practicing on theorem statements is surely a good place to start</p>\n</blockquote>\n<p>Actually <code>LeanAide</code> translates statements pretty well already. And many of the failure cases are because of lack of example terminology, which can be partly fixed by using these informalizations.</p>",
        "id": 436213809,
        "sender_full_name": "Siddhartha Gadgil",
        "timestamp": 1714463022
    },
    {
        "content": "<p>I agree with both the points about having brief definitions and informalizing to test. There are issues with resources though. For context, I informalized with the following prompt template, including relevant definitions (with docstrings if available):</p>\n<p><code>\"The following are some definitions in Lean 4 that are involved in the statement of a theorem which you have to describe in natural language.\\n```lean\\n${definitions}\\n```\\n\\nDescribe the following theorem (whose proof is suppressed for brevity) in natural language, similar to a documentation string. The description should be either a single sentence or a paragraph with 2-3 sentences, and may contain LaTeX mathematics.\\n```lean\\n${theorem}\\n```\\n\"</code></p>\n<p>If I queried too often GPT-4 throttled. So I had to add delays between queries and it took about a week to generate this data. Formalization needs larger prompts (examples of translations) so a full run will take about a month.</p>",
        "id": 436214262,
        "sender_full_name": "Siddhartha Gadgil",
        "timestamp": 1714463243
    },
    {
        "content": "<p>I feel the best bet for having more concise explanations is to use a smaller local model to shorten the GPT-4 descriptions, perhaps Mistral 7B or Lamma 3 8B. In general Mistral is much less chatty than GPT-4 so should be inclined to give briefer explanations.</p>",
        "id": 436214486,
        "sender_full_name": "Siddhartha Gadgil",
        "timestamp": 1714463346
    },
    {
        "content": "<p>I will try using Mistral unless someone has a better idea or someone with more ML skills and GPUs is trying something similar.</p>",
        "id": 436214679,
        "sender_full_name": "Siddhartha Gadgil",
        "timestamp": 1714463432
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"110038\">@Kevin Buzzard</span> I feel that we can already try to use machine translation a bit like the Mathlib3 port: with a machine step (or rather steps) followed by a human step. The machine step as of now will be weaker so much of the burden will be on humans. But over time productivity can go up.</p>\n<p>Further, ML can already do rewriting tasks well, for instance transforming proofs into a structured form an in this <a href=\"https://github.com/siddhartha-gadgil/LeanAide/blob/b462cdd44b0f20fa3fdceee31f77a243cf2c16d6/llm_data/gpt-4-turbo-preview/rational_fifth_power_identity/solve_with_expanded_deductions.json#L1147\">example</a>. The individual claims can then be autoformalized. Ideally the result should then be provable by <code>aesop</code> configured to use <code>ring</code> etc. </p>\n<p>In practice though there will be errors along the way to be fixed, and proofs that have to be hand completed.</p>\n<p>I feel part of the reason this has not been more developed is that you cannot make benchmarks for \"useful partial formalization\", and ML community is geared to targetting benchmarks.</p>",
        "id": 436220208,
        "sender_full_name": "Siddhartha Gadgil",
        "timestamp": 1714465416
    },
    {
        "content": "<p>Interesting comments!</p>\n<p>I'm finding with FLT that it's hard to point to the book and say \"this is the lemma we need\", because we tend to write things in a different generality.</p>",
        "id": 436225853,
        "sender_full_name": "Kevin Buzzard",
        "timestamp": 1714467650
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"284997\">@Zhangir Azerbayev</span> and <span class=\"user-mention\" data-user-id=\"110596\">@Rob Lewis</span>  did something similar with Codex in Lean 3.  See <a href=\"https://leanprover-community.github.io/mathlib_docs_demo/\">here</a> and #<strong>general&gt;Translation <em>to</em> natural language</strong> (specifically <a href=\"#narrow/stream/113488-general/topic/Translation.20*to*.20natural.20language/near/303052673\">this message</a>).  In their work, I noticed the descriptions tended to favor big famous theorems.  So for example a technical theorem about a Haar measure would be interpreted as some well-known result about a Haar measure.  The worst was theorems about coercions since that concept was completely foreign to the model.  I imagine there are probably good ways to separate good definitions from bad ones automatically, including some form of self-consistency with back translation (as Kim suggests above), but these approaches get harder as the models get more expensive to run.</p>",
        "id": 436269266,
        "sender_full_name": "Jason Rute",
        "timestamp": 1714482894
    },
    {
        "content": "<p>I don't know what their prompt was. What I did was delaborated the theorem statement to Syntax and included all the definitions with doc strings. So the model had something to work with. This may not have been so easy in Lean 3.</p>",
        "id": 436271217,
        "sender_full_name": "Siddhartha Gadgil",
        "timestamp": 1714483469
    },
    {
        "content": "<p>I think they just put in the pretty printed theorem (or maybe the Lean original code) with a few examples of translations, but I'm not positive.  Does your approach lead to better translations <span class=\"user-mention\" data-user-id=\"266304\">@Siddhartha Gadgil</span>?</p>",
        "id": 436271831,
        "sender_full_name": "Jason Rute",
        "timestamp": 1714483671
    },
    {
        "content": "<p>Also, I unfortunately see now that my link to their example (which was a clone of the mathlib docs but with AI generated descriptions) no longer contains the AI generated descriptions.  <span aria-label=\"sad\" class=\"emoji emoji-2639\" role=\"img\" title=\"sad\">:sad:</span></p>",
        "id": 436272086,
        "sender_full_name": "Jason Rute",
        "timestamp": 1714483747
    },
    {
        "content": "<p>I haven't compared with their's but in my own experiments with and without such information it did make a significant difference. Indeed these models are not great at \"recalling\" lean terminology but can pick it up from examples in the prompt fairly well.</p>",
        "id": 436272466,
        "sender_full_name": "Siddhartha Gadgil",
        "timestamp": 1714483864
    },
    {
        "content": "<p>Is your prompt adaptive to the specific theorem you are translating?</p>",
        "id": 436272630,
        "sender_full_name": "Jason Rute",
        "timestamp": 1714483916
    },
    {
        "content": "<p>I mean the examples in the prompt.  I assume you give some examples of good translations first, right?</p>",
        "id": 436272784,
        "sender_full_name": "Jason Rute",
        "timestamp": 1714483953
    },
    {
        "content": "<p>For informalization I don't give examples of translation. I instead give definitions, with doc strings if available, of the terms that appear in the syntax of the statement of the theorem. Since the target is English I don't think examples of that are needed.</p>\n<p>For formalization we give examples of related theorems and their translation. The hope is that these include most of the terms and syntax needed for the translation.</p>",
        "id": 436273253,
        "sender_full_name": "Siddhartha Gadgil",
        "timestamp": 1714484093
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"266304\">Siddhartha Gadgil</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/GPT.20generated.20theorem.20descriptions.2E/near/436214486\">said</a>:</p>\n<blockquote>\n<p>I feel the best bet for having more concise explanations is to use a smaller local model to shorten the GPT-4 descriptions, perhaps Mistral 7B or Lamma 3 8B. In general Mistral is much less chatty than GPT-4 so should be inclined to give briefer explanations.</p>\n</blockquote>\n<p>From my limited experience, mixtral 8x7B works surprisingly well for a model that runs reasonably fast in a (high end-ish) domestic GPU. But even before that, i would give a try to llam3-8B, since it is really fast (so, if it doesn't happen to work, you wouldn't lose much time trying).</p>\n<p>I just did a quick try with llama3. with just this prompt:</p>\n<div class=\"codehilite\" data-code-language=\"Lean\"><pre><span></span><code><span class=\"n\">The</span> <span class=\"n\">following</span> <span class=\"n\">is</span> <span class=\"n\">the</span> <span class=\"n\">statement</span> <span class=\"n\">of</span> <span class=\"n\">a</span> <span class=\"kd\">theorem</span> <span class=\"n\">writen</span> <span class=\"k\">in</span> <span class=\"n\">Lean4</span><span class=\"o\">,</span> <span class=\"n\">translate</span> <span class=\"n\">it</span> <span class=\"n\">to</span> <span class=\"n\">natural</span> <span class=\"n\">language</span><span class=\"o\">:</span> <span class=\"kd\">theorem</span> <span class=\"n\">AlgHom.map_sub</span> <span class=\"o\">:</span> <span class=\"bp\">∀</span> <span class=\"o\">{</span><span class=\"n\">R</span> <span class=\"o\">:</span> <span class=\"kt\">Type</span> <span class=\"n\">u</span><span class=\"o\">}</span> <span class=\"o\">{</span><span class=\"n\">A</span> <span class=\"o\">:</span> <span class=\"kt\">Type</span> <span class=\"n\">v</span><span class=\"o\">}</span> <span class=\"o\">{</span><span class=\"n\">B</span> <span class=\"o\">:</span> <span class=\"kt\">Type</span> <span class=\"n\">w</span><span class=\"o\">}</span> <span class=\"o\">[</span><span class=\"n\">inst</span> <span class=\"o\">:</span> <span class=\"n\">CommSemiring</span> <span class=\"n\">R</span><span class=\"o\">]</span> <span class=\"o\">[</span><span class=\"n\">inst_1</span> <span class=\"o\">:</span> <span class=\"n\">Ring</span> <span class=\"n\">A</span><span class=\"o\">]</span> <span class=\"o\">[</span><span class=\"n\">inst_2</span> <span class=\"o\">:</span> <span class=\"n\">Ring</span> <span class=\"n\">B</span><span class=\"o\">]</span> <span class=\"o\">[</span><span class=\"n\">inst_3</span> <span class=\"o\">:</span>  <span class=\"n\">Algebra</span> <span class=\"n\">R</span> <span class=\"n\">A</span><span class=\"o\">]</span> <span class=\"o\">[</span><span class=\"n\">inst_4</span> <span class=\"o\">:</span> <span class=\"n\">Algebra</span> <span class=\"n\">R</span> <span class=\"n\">B</span><span class=\"o\">]</span> <span class=\"o\">(</span><span class=\"n\">φ</span> <span class=\"o\">:</span> <span class=\"n\">A</span> <span class=\"bp\">→ₐ</span><span class=\"o\">[</span><span class=\"n\">R</span><span class=\"o\">]</span> <span class=\"n\">B</span><span class=\"o\">)</span> <span class=\"o\">(</span><span class=\"n\">x</span> <span class=\"n\">y</span> <span class=\"o\">:</span> <span class=\"n\">A</span><span class=\"o\">),</span> <span class=\"n\">φ</span> <span class=\"o\">(</span><span class=\"n\">x</span> <span class=\"bp\">-</span> <span class=\"n\">y</span><span class=\"o\">)</span> <span class=\"bp\">=</span> <span class=\"n\">φ</span> <span class=\"n\">x</span> <span class=\"bp\">-</span> <span class=\"n\">φ</span> <span class=\"n\">y</span>\n</code></pre></div>\n<p>It gave this response:</p>\n<div class=\"codehilite\" data-code-language=\"Lean\"><pre><span></span><code><span class=\"n\">Here's</span> <span class=\"n\">the</span> <span class=\"n\">translation</span><span class=\"o\">:</span>\n\n<span class=\"s2\">\"Theorem: For any types `R`, `A`, and `B` with a commutative semiring structure on `R`, ring structures on `A` and `B`, and algebra homomorphisms from `R` to `A` and from `R` to `B`, if `φ` is an algebra homomorphism from `A` to</span>\n<span class=\"s2\">`B`, then for any elements `x` and `y` in `A`, the image of their difference under `φ` is equal to the difference of their images under `φ`. In other words, `φ(x - y) = φ(x) - φ(y)`.\"</span>\n\n<span class=\"n\">Note</span> <span class=\"n\">that</span> <span class=\"n\">I've</span> <span class=\"n\">used</span> <span class=\"n\">more</span> <span class=\"n\">natural</span> <span class=\"n\">language</span> <span class=\"n\">than</span> <span class=\"n\">the</span> <span class=\"n\">original</span> <span class=\"kd\">theorem</span> <span class=\"n\">statement</span><span class=\"o\">,</span> <span class=\"n\">which</span> <span class=\"n\">is</span> <span class=\"n\">written</span> <span class=\"k\">in</span> <span class=\"n\">a</span> <span class=\"n\">more</span> <span class=\"n\">concise</span> <span class=\"n\">and</span> <span class=\"n\">formal</span> <span class=\"n\">style</span> <span class=\"n\">typical</span> <span class=\"n\">of</span> <span class=\"n\">mathematical</span> <span class=\"n\">proofs.</span>\n</code></pre></div>\n<p>in a couple of seconds.</p>",
        "id": 436323499,
        "sender_full_name": "Miguel Marco",
        "timestamp": 1714498727
    },
    {
        "content": "<p>mixtral 8x7b gave this in about seven seconds:</p>\n<div class=\"codehilite\" data-code-language=\"Lean\"><pre><span></span><code> <span class=\"n\">Given</span> <span class=\"n\">a</span> <span class=\"n\">commutative</span> <span class=\"n\">semiring</span> <span class=\"n\">R</span><span class=\"o\">,</span> <span class=\"n\">rings</span> <span class=\"n\">A</span> <span class=\"n\">and</span> <span class=\"n\">B</span><span class=\"o\">,</span> <span class=\"n\">and</span> <span class=\"n\">an</span> <span class=\"n\">algebra</span> <span class=\"n\">homomorphism</span> <span class=\"n\">φ</span> <span class=\"k\">from</span> <span class=\"n\">A</span> <span class=\"n\">to</span> <span class=\"n\">B</span><span class=\"o\">,</span> <span class=\"n\">for</span> <span class=\"n\">all</span> <span class=\"n\">elements</span> <span class=\"n\">x</span> <span class=\"n\">and</span> <span class=\"n\">y</span> <span class=\"k\">in</span> <span class=\"n\">A</span><span class=\"o\">,</span> <span class=\"n\">the</span> <span class=\"n\">homomorphism</span> <span class=\"n\">maps</span> <span class=\"n\">their</span> <span class=\"n\">difference</span> <span class=\"n\">to</span> <span class=\"n\">the</span> <span class=\"n\">difference</span> <span class=\"n\">of</span> <span class=\"n\">their</span> <span class=\"n\">images</span> <span class=\"n\">under</span> <span class=\"n\">φ</span><span class=\"o\">:</span> <span class=\"n\">that</span> <span class=\"n\">is</span><span class=\"o\">,</span> <span class=\"n\">φ</span><span class=\"o\">(</span><span class=\"n\">x</span> <span class=\"bp\">-</span> <span class=\"n\">y</span><span class=\"o\">)</span> <span class=\"bp\">=</span> <span class=\"n\">φ</span><span class=\"o\">(</span><span class=\"n\">x</span><span class=\"o\">)</span> <span class=\"bp\">-</span>\n<span class=\"n\">φ</span><span class=\"o\">(</span><span class=\"n\">y</span><span class=\"o\">)</span><span class=\"bp\">.</span>\n</code></pre></div>",
        "id": 436324415,
        "sender_full_name": "Miguel Marco",
        "timestamp": 1714499099
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"110087\">@Kim Morrison</span> <span class=\"user-mention\" data-user-id=\"110038\">@Kevin Buzzard</span>  I used Mistral 7B to rewrte the descriptions in more concise form. I have included the more concise descriptions italicised, for example in the <a href=\"https://math.iisc.ac.in/~gadgil/descs/Mathlib.Data.Nat.Prime.html\">Prime numbers page</a> . They seem reasonable.</p>\n<p><span class=\"user-mention\" data-user-id=\"461731\">@Miguel Marco</span> Thanks for the suggestions. I went with Mistral 7B for this task of making translations concise, because I do not have the resources to run Mistral 8x7b locally (I have one A4500 20GB CPU) and am having technical issues with stopping when running Llama 3 8B on vLLM (this is a known issue).</p>",
        "id": 436376230,
        "sender_full_name": "Siddhartha Gadgil",
        "timestamp": 1714524348
    },
    {
        "content": "<p>Which prompts did you use?</p>",
        "id": 436467561,
        "sender_full_name": "Miguel Marco",
        "timestamp": 1714552067
    },
    {
        "content": "<p>For summarization with Mistral 7B, this function:</p>\n<div class=\"codehilite\" data-code-language=\"Python\"><pre><span></span><code><span class=\"k\">def</span> <span class=\"nf\">prompt</span> <span class=\"p\">(</span><span class=\"n\">description</span><span class=\"p\">):</span>\n    <span class=\"n\">text</span> <span class=\"o\">=</span> <span class=\"sa\">f</span><span class=\"s2\">\"\"\"The following is a description of the statement of a theorem in Lean 4.</span>\n<span class=\"s2\">---</span>\n<span class=\"si\">{</span><span class=\"n\">description</span><span class=\"si\">}</span>\n<span class=\"s2\">---</span>\n<span class=\"s2\">Give a concise, single-sentence mathematical statement of the theorem. Give ONLY the statement</span>\n<span class=\"s2\">\"\"\"</span>\n    <span class=\"k\">return</span> <span class=\"n\">text</span>\n</code></pre></div>",
        "id": 436467710,
        "sender_full_name": "Siddhartha Gadgil",
        "timestamp": 1714552137
    },
    {
        "content": "<p>The original translation used relevant definitions in the template:  </p>\n<p>The following are some definitions in Lean 4 that are involved in the statement of a theorem which you have to describe in natural language.\\n<code>lean\\n${definitions}\\n</code>\\n\\nDescribe the following theorem (whose proof is suppressed for brevity) in natural language, similar to a documentation string. The description should be either a single sentence or a paragraph with 2-3 sentences, and may contain LaTeX mathematics.\\n<code>lean\\n${theorem}\\n</code>\\n</p>",
        "id": 436467835,
        "sender_full_name": "Siddhartha Gadgil",
        "timestamp": 1714552209
    },
    {
        "content": "<p>Hi! For the JSON file, what do the <code>name</code> and <code>frequency</code> entries denote? For example, here <br>\n<code>{\"name\": \"id\", \"frequency\": 37823}\n{\"name\": \"Eq.refl\", \"frequency\": 33722}\n{\"name\": \"Eq.mpr\", \"frequency\": 33681}</code></p>",
        "id": 437264931,
        "sender_full_name": "Shashank Pathak",
        "timestamp": 1715002944
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"364879\">Shashank Pathak</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/GPT.20generated.20theorem.20descriptions.2E/near/437264931\">said</a>:</p>\n<blockquote>\n<p>Hi! For the JSON file, what do the <code>name</code> and <code>frequency</code> entries denote? For example, here <br>\n<code>{\"name\": \"id\", \"frequency\": 37823}\n{\"name\": \"Eq.refl\", \"frequency\": 33722}\n{\"name\": \"Eq.mpr\", \"frequency\": 33681}</code></p>\n</blockquote>\n<p>The name of the theorem and the frequency with which it appears in <strong>term</strong> proofs.</p>",
        "id": 437287948,
        "sender_full_name": "Siddhartha Gadgil",
        "timestamp": 1715009795
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"110038\">Kevin Buzzard</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/GPT.20generated.20theorem.20descriptions.2E/near/436206914\">said</a>:</p>\n<blockquote>\n<p>This would be more likely to work the more verbose the translation, right?</p>\n<p>I think that formalising these informal statements sounds like a great idea! If we can get machines translating latex to lean then I can get FLT done much quicker. Right now I guess it's science fiction but practicing on theorem statements is surely a good place to start</p>\n</blockquote>\n<p>Personally I believe it's not a science fiction but a within reach problem that's going to be fundamentally resolved in the coming years. Statistically speaking, the task is not difficult at all. We need some good AI system technique that guarantees both accuracy and speed.</p>",
        "id": 446778868,
        "sender_full_name": "Xiyu Zhai",
        "timestamp": 1719292342
    },
    {
        "content": "<p>I believe there should be a systematic way of evaluating trustworthiness of autoformalization. Formalized proof in Lean is 100% correct, then the correctness of autoformalization dominates the error.</p>",
        "id": 446779068,
        "sender_full_name": "Xiyu Zhai",
        "timestamp": 1719292477
    },
    {
        "content": "<p>I suggest a Chain-Of-Though approach for complicated translation. I'm working on it. But it shall take about a year.</p>",
        "id": 446779128,
        "sender_full_name": "Xiyu Zhai",
        "timestamp": 1719292514
    },
    {
        "content": "<p>There should be a hybrid AI system that easily incorporates new Lean formalized concepts into itself and runs with optimal speed yet gives sound guarantees.</p>",
        "id": 446779365,
        "sender_full_name": "Xiyu Zhai",
        "timestamp": 1719292678
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"466120\">@Xiyu Zhai</span> I look forward to seeing what you will come up with!</p>\n<p>As for what is and isn't currently science fiction, that is probably a matter of degree (and how much you are factoring in future research).</p>\n<ul>\n<li>Translating self-contained mathematical statements that use standard mathematical objects into Lean or Isabelle seems to be making good progress.  This includes using good feedback and reinforcement learning loops like in the recent DeepSeek-Prover (<a class=\"stream-topic\" data-stream-id=\"219941\" href=\"/#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/DeepSeek-Prover\">#Machine Learning for Theorem Proving &gt; DeepSeek-Prover</a>), which is probably state-of-the-art here.</li>\n<li>It is harder once the math concepts are more esoteric even if they already are found in a library like Mathlib.  (Although using RAG and Lean's metaprogramming like in LeanAid are probably good approaches.)</li>\n<li>It is much harder once one has to translate new definitions.  Now you really have to start to understand the concepts involved better, especially if the translation isn't routine.  I'm not aware of any work in this direction.</li>\n<li>Translating proofs is also more of a challenge.</li>\n<li>I think once we start talking about translating an article, textbook, or research book, then we really are at least several years, if not decades, into the future.  Current deep learning tools just don't do high-level organization and planning yet.  It is the same difference as having ChatGPT write a few lines of code versus plan and develop a whole large project. (Although I could see auto-formalization being one of the first successful applications of such a project-level system since a formal theorem prover provides so much feedback and reward signals to help the agent know it is on the right path.)</li>\n</ul>",
        "id": 447021781,
        "sender_full_name": "Jason Rute",
        "timestamp": 1719359149
    },
    {
        "content": "<ul>\n<li>And even if we could translate very well-written, self-contained articles or textbooks, real math is incredibly messy: citations, mistakes, left as an exercise to the reader, abuse of notation and terminology, lazy writing, the reader needs time to develop their understanding and intuition about the field, etc.  That would be even another order of magnitude more difficult.  However, this might have an interesting effect if we tell mathematician that our AI could formalize and check their work as long as it is well-written. :)</li>\n</ul>",
        "id": 447023170,
        "sender_full_name": "Jason Rute",
        "timestamp": 1719359565
    },
    {
        "content": "<p>I have a new programming language for writing AI agents, the value of which I want to demonstrate in this endeavor.  (In simple terms, in the language one easily breaks a complicated tasks into smaller ones, and using larger llms as labeling functions we can train smaller llms to achieve these simple tasks. The complete story is much more convoluted though)</p>\n<p>The points you gave here are invaluable. It gives me a clearer idea of what difficulties lie in here. I'll definitely cite this. (I don't know what's the right way to quote a discussion in zulip though, but sometimes I feel the information in zulip can be much more valuable than those in papers)</p>",
        "id": 447054681,
        "sender_full_name": "Xiyu Zhai",
        "timestamp": 1719373092
    },
    {
        "content": "<p>From a theoretical viewpoint, I want to understand what's the computation process of translating a human language proof to a formalized one, and how to decompose it properly so that it can be reliably solved by an AI system with as little human intervention as possible.</p>",
        "id": 447055185,
        "sender_full_name": "Xiyu Zhai",
        "timestamp": 1719373268
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"266304\">@Siddhartha Gadgil</span> Is there an easy way I can use your auto-informalization  on my own theorems and definitions? </p>\n<p>Ideally I want to be able to run an  auto-informlalizer on all of the lemmas and definitions in a file, certify them individually (as a human) as good or bad (or better or worse then the existing doc-string if it exists), and place the good ones in the file as a doc-strings.</p>",
        "id": 447287394,
        "sender_full_name": "Joseph Tooby-Smith",
        "timestamp": 1719424143
    },
    {
        "content": "<p>It is open source in LeanAide but currently tangled in other code. I will try to make an easy to use version</p>",
        "id": 447336875,
        "sender_full_name": "Siddhartha Gadgil",
        "timestamp": 1719435947
    }
]