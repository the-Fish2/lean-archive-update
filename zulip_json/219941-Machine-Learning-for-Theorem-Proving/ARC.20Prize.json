[
    {
        "content": "<p>The ARC benchmark now comes with a $1 million prize.  <a href=\"https://news.ycombinator.com/item?id=40648960\">https://news.ycombinator.com/item?id=40648960</a><br>\nWe have talked about this benchmark before on <a class=\"stream-topic\" data-stream-id=\"219941\" href=\"/#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/dreamcoder\">#Machine Learning for Theorem Proving &gt; dreamcoder</a></p>",
        "id": 444231261,
        "sender_full_name": "Jason Rute",
        "timestamp": 1718197397
    },
    {
        "content": "<p>The ARC problems are a form of visual intelligence tests.  If you have never seen them, you can try them out here for yourself:</p>\n<ul>\n<li><a href=\"https://arcprize.org/play\">https://arcprize.org/play</a> (Seems to be the official playground.)</li>\n<li><a href=\"https://volotat.github.io/ARC-Game/\">https://volotat.github.io/ARC-Game/</a> (An older non-official interface that gives you the final board size.)</li>\n</ul>",
        "id": 444233639,
        "sender_full_name": "Jason Rute",
        "timestamp": 1718197923
    },
    {
        "content": "<p>pretty similar in format to the AIMO prize, it seems :)</p>",
        "id": 444250443,
        "sender_full_name": "Junyan Xu",
        "timestamp": 1718202477
    },
    {
        "content": "<p>Here is an interesting podcast/video interview with Francois Chollet about the prize: <a href=\"https://www.youtube.com/watch?v=UakqL6Pj9xo\">https://www.youtube.com/watch?v=UakqL6Pj9xo</a></p>\n<div class=\"youtube-video message_inline_image\"><a data-id=\"UakqL6Pj9xo\" href=\"https://www.youtube.com/watch?v=UakqL6Pj9xo\"><img src=\"https://uploads.zulipusercontent.net/55318996cc27f6d21eef0b80e4a9f71847b22097/68747470733a2f2f692e7974696d672e636f6d2f76692f55616b714c36506a39786f2f64656661756c742e6a7067\"></a></div><p>Also, here is the Kaggle competition: <a href=\"https://www.kaggle.com/c/arc-prize-2024\">https://www.kaggle.com/c/arc-prize-2024</a></p>",
        "id": 444581105,
        "sender_full_name": "Jason Rute",
        "timestamp": 1718321021
    },
    {
        "content": "<p>Here's a claim of 50% result on ARC-AGI problem (74% when compared with the subset of the data that have a human baseline of 85%).</p>\n<p>It uses chatGPT-4o (and reportedly a lot of RAM), so is not eligible for the prize:</p>\n<p><a href=\"https://redwoodresearch.substack.com/p/getting-50-sota-on-arc-agi-with-gpt\">https://redwoodresearch.substack.com/p/getting-50-sota-on-arc-agi-with-gpt</a></p>\n<p>This is a blog post and not a research article, but the source code is provided, so it could be in principle reproduced (although probably a little too costly for somebody without a research budget):</p>\n<p><a href=\"https://github.com/rgreenblatt/arc_draw_more_samples_pub\">https://github.com/rgreenblatt/arc_draw_more_samples_pub</a></p>",
        "id": 445280182,
        "sender_full_name": "Adam Kurkiewicz",
        "timestamp": 1718692022
    },
    {
        "content": "<p>See also <a href=\"https://x.com/dwarkesh_sp/status/1802771055016378554\">https://x.com/dwarkesh_sp/status/1802771055016378554</a></p>",
        "id": 445287198,
        "sender_full_name": "Junyan Xu",
        "timestamp": 1718695005
    },
    {
        "content": "<p>I'm wondering how it's appropriate to leak 100 test problems to OpenAI, giving them unfair advantage (e.g. seed data to generate synthetic ones).</p>",
        "id": 445288549,
        "sender_full_name": "Junyan Xu",
        "timestamp": 1718695514
    },
    {
        "content": "<p>It's not completely clear to me what's the meaning of \"train\" \"public train\" and \"test\" here.</p>\n<p>I think the ARC problems in the kaggle competition are completely hidden? And this hasn't run on the hidden test, but only on the \"public test\"?</p>",
        "id": 445299412,
        "sender_full_name": "Adam Kurkiewicz",
        "timestamp": 1718698765
    },
    {
        "content": "<p>One thing Chollet says in the interview above is that if you want to test on GPT-4, then the best way is probably to make some of your own completely new hidden test problems (not hard problems, but completely new problems), and try it on those.</p>",
        "id": 445329684,
        "sender_full_name": "Jason Rute",
        "timestamp": 1718708958
    },
    {
        "content": "<p>Good point <span class=\"user-mention\" data-user-id=\"115715\">@Jason Rute</span>: it's hard work coming up with those though (in sufficient number)!</p>\n<p>I want to do a benchmark for lean autoformalisation (let's say LAB: Lean Autoformalisation Benchmark), where the model would be supplied with complete (and detailed) natural language for one of the IMO problems and the formal statement of the problem (I've been formalising some for Compfiles), with <code>sorry</code> as the proof to fill in and it would have to come up with the formalisation of the solution. I would completely hide the \"private test\" behind kaggle (although in principle, because this will be a previously described IMO problem, there is of course a risk that somebody could formalise it, e.g. for Compfiles, and that could then leak).</p>\n<p>But it will be a lot of work to do this properly, even for a very small number of problems (like 4 or 6). Formalising maths is just really laborious!</p>\n<p>Another thing that I'll have to work out is how to run the submissions \"adversarially\", checking the axioms, etc... there's probably a lean flag for this somewhere.</p>",
        "id": 445386768,
        "sender_full_name": "Adam Kurkiewicz",
        "timestamp": 1718724012
    },
    {
        "content": "<blockquote>\n<p>I think the ARC problems in the kaggle competition are completely hidden? And this hasn't run on the hidden test, but only on the \"public test\"?</p>\n</blockquote>\n<p>I think you're right. I just read the blog post and the author says in the first sentence that the 50% accuracy is from the public test set, presumably the 400 evaluation tasks in the <a href=\"https://github.com/fchollet/ARC-AGI#task-file-format\">repo</a> (I read on X that the author tested on 100 tasks).  Later in the post he says \"The prior state of the art on this dataset was 34% accuracy\" with a link to the <a href=\"https://arcprize.org/leaderboard\">ARC Prize Leaderboard</a> (not Kaggle leaderboard), which reports performance on the private test set, but <a href=\"https://redwoodresearch.substack.com/p/getting-50-sota-on-arc-agi-with-gpt#footnote-3-145731248\">footnote 3</a> says it doesn't matter because the private test set is supposed to be IID with the public test set. Footnote 2 says the author hasn't submitted to the ARC Prize Leaderboard, so his method hasn't been tested on the private test set yet (and testing on the private set <a href=\"https://arcprize.org/leaderboard\">won't allow Internet access</a>).</p>",
        "id": 445416155,
        "sender_full_name": "Junyan Xu",
        "timestamp": 1718730811
    },
    {
        "content": "<p>I had the question after reading <a href=\"https://x.com/bshlgrs/status/1802766449075003770\">https://x.com/bshlgrs/status/1802766449075003770</a>:</p>\n<blockquote>\n<p>Train set: 71% vs a human baseline of 85%<br>\nTest set: 51% vs prior SoTA of 34% (human baseline is unknown)<br>\n(The train set is much easier than the test set.)<br>\n(These numbers are on a random subset of 100 problems that we didn't iterate on.)</p>\n</blockquote>\n<p>which is also implying the accuracy on the public test set is representative of that on the private test set. However on Hacker News](<a href=\"https://news.ycombinator.com/item?id=40712282\">https://news.ycombinator.com/item?id=40712282</a>) Mike Knoop has pointed out the assumption is false:</p>\n<blockquote>\n<p>this result is on the public eval set vs private set (ARC Prize $).<br>\nthe current private set SOTA ~35% solution also performed ~50% on the public set. so this new result might be SOTA but hasn't been validated or scrutinized yet.</p>\n</blockquote>\n<p>So apparently the public test set is harder than the training set, and the private test set is still harder.<br>\n<strong>Update</strong>: but this contradicts the <a href=\"https://arcprize.org/guide#data-structure\">official guide</a>: \"The public evaluation sets and the private test sets are intended to be the same difficulty.\"</p>\n<p>Human accuracy on the training set <a href=\"https://arcprize.org/guide#grand-prize-goal\">is 84%</a> according to a 2021 New York University study, and the grand prize goal is set at 85%. I guess no similar study has been conducted on the public test set yet, but it might soon be. Chollet commented on human performance <a href=\"https://x.com/fchollet/status/1801773350613828066\">on X</a>:</p>\n<blockquote>\n<p>The private test set was checked for feasibility by two people, who had never seen the tasks before. They successfully solved 100% of the tasks. I would expect a smart person (&gt;1 sigma) to be able to do &gt;95%, and two or more should do 100%. At 2 sigma you should score 100% easily.<br>\nARC-AGI isn't intended to be hard.<br>\nThe bar to \"solve\" ARC-AGI is only 85%. We think this roughly aligns with average human performance, although this hasn't been rigorously measured on the private test set (being private, it's preferable to restrict the number of people who have seen it)</p>\n</blockquote>\n<p>Given that the private test set is a lot harder than the training set for machines, it seems odd to expect an average human to perform similarly on both sets ...</p>",
        "id": 445432661,
        "sender_full_name": "Junyan Xu",
        "timestamp": 1718735668
    },
    {
        "content": "<p>BTW, Chollet has now made two posts on X bullish of the prospects of program synthesis on solving reasoning (I don't remember when I last liked his posts but I just liked both of these):<br>\n<a href=\"https://x.com/fchollet/status/1803096195684012371\">https://x.com/fchollet/status/1803096195684012371</a><br>\n<a href=\"https://x.com/fchollet/status/1802801425514410275\">https://x.com/fchollet/status/1802801425514410275</a><br>\nand said <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/dreamcoder\">DreamCoder</a> was once his favorite paper:<br>\n<a href=\"https://x.com/fchollet/status/1800647127988670510\">https://x.com/fchollet/status/1800647127988670510</a><br>\n(he actually <a href=\"https://x.com/fchollet/status/1489066366392930307\">recomended it in 2022</a>)</p>",
        "id": 445434103,
        "sender_full_name": "Junyan Xu",
        "timestamp": 1718736145
    },
    {
        "content": "<p>I remember chuckling when I read this in the blog post. How are you supposed to put a distribution on something that is supposed to be unique (each ARC problem is supposed to be \"outside\" of the distribution, so completely novel reasoning)?</p>\n<blockquote>\n<p>because the private test set is supposed to be IID with the public test set.</p>\n</blockquote>",
        "id": 445436654,
        "sender_full_name": "Adam Kurkiewicz",
        "timestamp": 1718736960
    },
    {
        "content": "<p>I think there are three sets.  If you go to <a href=\"https://arcprize.org/play\">https://arcprize.org/play</a> you see there is “public evaluation set (hard)” and “public training set (easy)” and I’m almost sure neither is the private set used for testing in Kaggle.  I think when the medium post talks about the part of the dataset where humans get 85% and they get in the 70%s they mean the “public training set (easy)”, but I could be mistaken.</p>",
        "id": 445465477,
        "sender_full_name": "Jason Rute",
        "timestamp": 1718746316
    },
    {
        "content": "<p>(Oh sorry, I didn’t read your message clearly <span class=\"user-mention\" data-user-id=\"224323\">@Junyan Xu</span>.  You point all this out already.)</p>",
        "id": 445465826,
        "sender_full_name": "Jason Rute",
        "timestamp": 1718746462
    }
]