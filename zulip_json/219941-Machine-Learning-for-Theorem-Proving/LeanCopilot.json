[
    {
        "content": "<p>We introduce <a href=\"https://github.com/lean-dojo/LeanCopilot\">Lean Copilot</a> for language models to suggest tactics, search for proofs, and select premises in Lean.  Here is a short demo:</p>\n<p><a href=\"/user_uploads/3121/vEiWfN_3QfOGt6RcYIgPSr1m/Lean-Copilot-v1.mp4\">Lean-Copilot-v1.mp4</a></p>\n<div class=\"message_inline_image message_inline_video\"><a href=\"/user_uploads/3121/vEiWfN_3QfOGt6RcYIgPSr1m/Lean-Copilot-v1.mp4\" title=\"Lean-Copilot-v1.mp4\"><video preload=\"metadata\" src=\"/user_uploads/3121/vEiWfN_3QfOGt6RcYIgPSr1m/Lean-Copilot-v1.mp4\"></video></a></div><p>Its initial version was known as LeanInfer, but now we have significantly expanded its scope:</p>\n<ul>\n<li>Faster tactic generation on both CPUs and GPUs (powered by <a href=\"https://github.com/OpenNMT/CTranslate2\">CTranslate2</a>)</li>\n<li>Higher tactic generation quality, using beam search instead of multinomial sampling.</li>\n<li>Integrating LLM-generated tactics with <a href=\"https://github.com/leanprover-community/aesop\">aesop</a> for proof search.</li>\n<li>Retrieval for premise selection</li>\n<li>Supporting user-provided models.</li>\n</ul>",
        "id": 407175937,
        "sender_full_name": "Kaiyu Yang",
        "timestamp": 1702278471
    },
    {
        "content": "<p>This is probably a stupid question, but how far are we from being able to use something like this to \"clean up\" some code and it get PR ready or close to PR ready? I have a bunch of stuff in a repo I need to PR and while I don't expect something like this to be able to prove the results it contains, it would be nice if it could learn from the proofs that are in the repo and then just tidy it up (i.e.  I say this is the main result I want you to prove, then go, find the proof in the repo, break up into smaller results, remove unused results, fix spacing/layout etc).  Perhaps I'm just way behind and this is already possible, or maybe  this is sci fi and its actually hard to do and we are not there yet.</p>",
        "id": 407216529,
        "sender_full_name": "Chris Birkbeck",
        "timestamp": 1702292560
    },
    {
        "content": "<p>(also sorry if this is off topic for this thread, it was just that the premise selection and proof search bits got me thinking about this)</p>",
        "id": 407216899,
        "sender_full_name": "Chris Birkbeck",
        "timestamp": 1702292673
    },
    {
        "content": "<p>One danger of putting the discussion here is that if this is really meant as a user facing tool with good usability (easy to install, no GPU needed, etc), then you may be missing much of your audience.  It so, it might be worthy of a post in <a class=\"stream\" data-stream-id=\"113486\" href=\"/#narrow/stream/113486-announce\">#announce</a> with a link back here.</p>",
        "id": 407226967,
        "sender_full_name": "Jason Rute",
        "timestamp": 1702296556
    },
    {
        "content": "<p>I see you have a short paper on this at Math-AI 2023 (<a href=\"https://mathai2023.github.io/papers/4.pdf\">https://mathai2023.github.io/papers/4.pdf</a>).  I’m curious if this system could be used for AI research, especially with the build your own model capabilities.  In particular one would need more baselines.  You only test on 50 theorems and they are from Mathematics in Lean.  Why?  This is just another incomparable baseline to all the others out there in the literature.  I’m also curious about your test.  Was it a full proof search?  Was it a deterministic timeout?  Either way, how long did it typically take to run per theorem.  Did you use a GPU?  And if so, how long would it take per theorem on a CPU?</p>",
        "id": 407228827,
        "sender_full_name": "Jason Rute",
        "timestamp": 1702297209
    },
    {
        "content": "<p>I’m also curious how this compares to LLMStep by <span class=\"user-mention\" data-user-id=\"409334\">@Sean Welleck</span> both in terms of capabilities and in terms of ease of use.  I guess for both, it would be best if users play with it, but I’m not sure if either is designed for end users yet.</p>",
        "id": 407229311,
        "sender_full_name": "Jason Rute",
        "timestamp": 1702297386
    },
    {
        "content": "<p>And how it compares to published works like your own ReProver.  Is it significantly nerfed?</p>",
        "id": 407229567,
        "sender_full_name": "Jason Rute",
        "timestamp": 1702297455
    },
    {
        "content": "<p>Lean 3 used to have the ability (maybe still does) to just do the tactic suggestions automatically in the infoview (using HTPS model via an API).  That would be a much better user interface than typing a tactic every time.  That is if it doesn’t churn through your CPU too much.  <span class=\"user-mention\" data-user-id=\"110043\">@Gabriel Ebner</span> added it to Lean 3.</p>",
        "id": 407232940,
        "sender_full_name": "Jason Rute",
        "timestamp": 1702298706
    },
    {
        "content": "<p>Although the Lean 3 infoview suggestions don’t run or rank the tactics which I think this does.  That is even more important than putting them in the infoview.  (I gave a demo of the Lean 3 tactic suggestions at the IPAM workshop once and the audience was really skeptical since all the first suggested tactics failed.)</p>",
        "id": 407236648,
        "sender_full_name": "Jason Rute",
        "timestamp": 1702299962
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"389019\">Chris Birkbeck</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/LeanCopilot/near/407216529\">said</a>:</p>\n<blockquote>\n<p>This is probably a stupid question, but how far are we from being able to use something like this to \"clean up\" some code and it get PR ready or close to PR ready? I have a bunch of stuff in a repo I need to PR and while I don't expect something like this to be able to prove the results it contains, it would be nice if it could learn from the proofs that are in the repo and then just tidy it up (i.e.  I say this is the main result I want you to prove, then go, find the proof in the repo, break up into smaller results, remove unused results, fix spacing/layout etc).  Perhaps I'm just way behind and this is already possible, or maybe  this is sci fi and its actually hard to do and we are not there yet.</p>\n</blockquote>\n<p>We think it would be interesting to have something that can learn from your repo (as long as it's public on GitHub and can be built via <code>lake build</code>), automatically fill in <code>sorry</code> and submit pull requests to your repo. I don't see a fundamental difficulty here. It's just that we have limited personnel and have to priortize.</p>\n<p>BTW, I was wondering if \"PR\" here means pull requests or public relations <span aria-label=\"rolling on the floor laughing\" class=\"emoji emoji-1f923\" role=\"img\" title=\"rolling on the floor laughing\">:rolling_on_the_floor_laughing:</span>.</p>",
        "id": 407793919,
        "sender_full_name": "Kaiyu Yang",
        "timestamp": 1702494798
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"115715\">Jason Rute</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/LeanCopilot/near/407226967\">said</a>:</p>\n<blockquote>\n<p>worthy of a post in #announce with a link back here.</p>\n</blockquote>\n<p>Thanks for the suggestion! I'll do that once I get a chance.</p>\n<blockquote>\n<p>short paper on this at Math-AI 2023 </p>\n</blockquote>\n<p>That paper describes an earlier version of Lean Copilot (around September) and hasn't been updated. We plan to release a full paper in early 2024. We haven't decided whether to submit it to a ML conference or conferences like ITP. Personally I lean towards ITP since we want the main contribution to be a user-facing tool.</p>",
        "id": 407794036,
        "sender_full_name": "Kaiyu Yang",
        "timestamp": 1702494847
    },
    {
        "content": "<blockquote>\n<p>You only test on 50 theorems and they are from Mathematics in Lean. Why? This is just another incomparable baseline to all the others out there in the literature. I’m also curious about your test. Was it a full proof search? Was it a deterministic timeout? Either way, how long did it typically take to run per theorem. Did you use a GPU? And if so, how long would it take per theorem on a CPU?</p>\n</blockquote>\n<blockquote>\n<p>And how it compares to published works like your own ReProver. Is it significantly nerfed?</p>\n</blockquote>\n<p>Just to clarify, Lean Copilot does not introduce new models or algorithms for machine learning to prove theorems. You can think of it as a \"frontend\" that interfaces with existing methods. Under the hood, it uses the same ReProver (w/o retrieval) model as in LeanDojo (and you can bring your own models). Therefore, I don't think it's necessary to evaluate Lean Copilot empirically on benchmarks. The results should just be similar to what we described in the LeanDojo paper, despite some minor differences, e.g., different implementations of beam search by CTranslate2 and Hugging Face.</p>",
        "id": 407794835,
        "sender_full_name": "Kaiyu Yang",
        "timestamp": 1702495263
    },
    {
        "content": "<p>From the user side, I’m wondering how much worse performance gets for a typical user computer versus your LeanDojo evaluation machine with eight GPUs.  But it is also true the real test is what <span class=\"user-mention\" data-user-id=\"110038\">@Kevin Buzzard</span> or other Lean users think.</p>",
        "id": 407797388,
        "sender_full_name": "Jason Rute",
        "timestamp": 1702496401
    },
    {
        "content": "<p>As for the ML side, we need better tools for AI researchers (like me) to experiment with new ideas without having to rebuild everything from scratch.  Since you can plug in your own models here, this might be one such tool.  (And in another thread you suggested that.)  If so, it would be good to have baselines and ways to run those baselines.  [Edit: One advantage of using this system as a research tool is that research experiments can be automatically distributed as user-facing tools right away.]</p>",
        "id": 407797476,
        "sender_full_name": "Jason Rute",
        "timestamp": 1702496423
    },
    {
        "content": "<p>But I also think I’ve been too negative.  Great job, and I hope the Lean community finds this to be a valuable tool which is easy to use and powerful enough to be useful.  I don’t know if there are any such tools yet in ITP.  (Maybe SledgeHammer, CoqHammer, and Tactician come closest and I’d love an informal comparison with those from users familiar with the other tools.)</p>",
        "id": 407797908,
        "sender_full_name": "Jason Rute",
        "timestamp": 1702496598
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"115715\">Jason Rute</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/LeanCopilot/near/407797388\">said</a>:</p>\n<blockquote>\n<p>But it is also true the real test is what <span class=\"user-mention silent\" data-user-id=\"110038\">Kevin Buzzard</span> or other Lean users think.</p>\n</blockquote>\n<p>I playfully (!) pick on Kevin here since I think he is the most vocal critic of the current research, but I know <span class=\"user-mention\" data-user-id=\"260507\">@Heather Macbeth</span>, <span class=\"user-mention\" data-user-id=\"112680\">@Johan Commelin</span>, <span class=\"user-mention\" data-user-id=\"110087\">@Scott Morrison</span>, <span class=\"user-mention\" data-user-id=\"110865\">@Jeremy Avigad</span> and others have also expressed that they don't have a chance to play with the published AI work, so I hope this is not only a chance for them to play with it, but to tell us AI researchers what these systems still need to go the extra mile (or 10 or 100) both in terms of usability and capabilities.</p>",
        "id": 407799830,
        "sender_full_name": "Jason Rute",
        "timestamp": 1702497371
    },
    {
        "content": "<p>When calling <code>lake update LeanCopilot</code> I get the error</p>\n<blockquote>\n<p>error: no error (error code : 0)</p>\n</blockquote>\n<p>that has the effect that the next command <code>lake exe LeanCopilot/download</code> fails.</p>",
        "id": 407800251,
        "sender_full_name": "Filippo A. E. Nuccio",
        "timestamp": 1702497542
    },
    {
        "content": "<p>Oh, I see; I must be in Win WSL and not in Win itself.</p>",
        "id": 407800474,
        "sender_full_name": "Filippo A. E. Nuccio",
        "timestamp": 1702497624
    },
    {
        "content": "<p>2 messages were moved here from <a class=\"stream-topic\" data-stream-id=\"113486\" href=\"/#narrow/stream/113486-announce/topic/LeanCopilot\">#announce &gt; LeanCopilot</a> by <span class=\"user-mention silent\" data-user-id=\"110596\">Rob Lewis</span>.</p>",
        "id": 407800618,
        "sender_full_name": "Notification Bot",
        "timestamp": 1702497671
    },
    {
        "content": "<p>That message does get printed in Windows. It seems to happen when Lake and/or Lean is already running (as a language server in VS Code, for example).</p>",
        "id": 407801213,
        "sender_full_name": "Richard Copley",
        "timestamp": 1702497856
    },
    {
        "content": "<p>I reported that <a href=\"#narrow/stream/270676-lean4/topic/Lake.20on.20Windows/near/405193754\">here</a>. The conversation tailed off.</p>",
        "id": 407801639,
        "sender_full_name": "Richard Copley",
        "timestamp": 1702498054
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"300245\">Filippo A. E. Nuccio</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/LeanCopilot/near/407800474\">said</a>:</p>\n<blockquote>\n<p>Oh, I see; I must be in Win WSL and not in Win itself.</p>\n</blockquote>\n<p>Yep, Windows does not work yet. See <a href=\"https://github.com/lean-dojo/LeanCopilot/issues/31\">https://github.com/lean-dojo/LeanCopilot/issues/31</a></p>",
        "id": 407801754,
        "sender_full_name": "Kaiyu Yang",
        "timestamp": 1702498098
    },
    {
        "content": "<p>I see, no problem. I have WSL installed (with an oldish version of lake and little time to update now), but I will be happy to test on Win whenever it will become available.</p>",
        "id": 407802484,
        "sender_full_name": "Filippo A. E. Nuccio",
        "timestamp": 1702498406
    },
    {
        "content": "<p>I'll rise to the bait here. Come April (when teaching is out of the way) I hope to be spending a lot of time proving Fermat's Last Theorem. What can this system offer me? I tend to just ignore all the messages in this stream because my instinct is that right now they can offer me nothing. But I'd be very happy to be proved wrong. In practice I'm going to be translating technical mathematics from research papers into lean.</p>",
        "id": 407808470,
        "sender_full_name": "Kevin Buzzard",
        "timestamp": 1702501016
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"110038\">@Kevin Buzzard</span> see the 2 min video at the top of this thread.  Previous works like lean-gptf had tactic suggestions, but not full proof search or premise selection for end users.  (I’m not involved in the work, but) I’m curious if you think the tactic predictions and/or proofs it finds are any good.  I’m also curious of if you think tools like this would be useful enough if say they had better user interfaces (automatic tactic/premise suggestions, the ability to search for proofs in the background, etc.)  In short, I’m curious where the state of this field is at from the point of view of end users.</p>",
        "id": 407812285,
        "sender_full_name": "Jason Rute",
        "timestamp": 1702502452
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"584504\">@Kaiyu Yang</span> Does premises selection include new premises, or just those seen during training?</p>",
        "id": 407812393,
        "sender_full_name": "Jason Rute",
        "timestamp": 1702502503
    },
    {
        "content": "<p>I guess I watch these videos and feel like they are so far away from my actual use case, but really what I should do is just try to use the software I guess.</p>",
        "id": 407813332,
        "sender_full_name": "Kevin Buzzard",
        "timestamp": 1702502928
    },
    {
        "content": "<p>And say what your use case is…, maybe in its own thread or a blog post even after trying these tools.</p>",
        "id": 407813707,
        "sender_full_name": "Jason Rute",
        "timestamp": 1702503091
    },
    {
        "content": "<p>I am mildly disappointed that it doesn't seem to work like GitHub Copilot. Is there a way just let my cursor sit where I need to complete my proof and have it automagically finish it for me if it can?</p>",
        "id": 407813883,
        "sender_full_name": "Bolton Bailey",
        "timestamp": 1702503161
    },
    {
        "content": "<p>Another minor complaint: The installation seems to indicate that I need to import the project as a Lean dependency. This makes it hard to contribute to Mathlib with this, since I generally just work in the Mathlib repository itself, and I would need to modify Mathlib's lakefile to import this, but then not commit those modifications.</p>",
        "id": 407814534,
        "sender_full_name": "Bolton Bailey",
        "timestamp": 1702503448
    },
    {
        "content": "<p>(I guess I'll just make some feature suggestions)</p>",
        "id": 407814758,
        "sender_full_name": "Bolton Bailey",
        "timestamp": 1702503569
    },
    {
        "content": "<p>OpenAI has spoiled everyone.  Haha.  But I imagine part of this is just UI and the Lean community could help build better UI for this sort of thing around the existing tech.  Avi Shinnar had <a href=\"https://vimeo.com/831643416#t=38m50s\">some good takes on what a good UI would look like at the recent National Academies meeting</a> (at 38:50 mins in).</p>",
        "id": 407815038,
        "sender_full_name": "Jason Rute",
        "timestamp": 1702503699
    },
    {
        "content": "<p>The Mathlib comment was the number one reason no one ever used Lean GPT-f, so it is probably a big deal.</p>",
        "id": 407815160,
        "sender_full_name": "Jason Rute",
        "timestamp": 1702503742
    },
    {
        "content": "<p>Actually, maybe that wasn't quite the same, since lean-gptf required mathlib so it couldn't be used for mathlib development, where I guess you are just saying it is annoying to use for mathlib development.</p>",
        "id": 407815705,
        "sender_full_name": "Jason Rute",
        "timestamp": 1702503972
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"584504\">@Kaiyu Yang</span> I remarked on this before, but I think it's a bit more important now that it's a tactic and not just a python API for lean: your tactic <code>select_premises</code> is misnamed, these are not 'premises', they are theorems and lemmas in lean terminology. (I'm aware that this terminology is used esp. in the ATP field, but it makes a bit more sense there given that all premises are taken as axioms or hypotheses when proving the conjecture in question. In the context of lean/mathlib, these are all proved theorems.)</p>",
        "id": 407817086,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1702504576
    },
    {
        "content": "<p>“Lemma selection” seems to be the second most common term for this in the literature, so maybe <code>select_lemmas</code>?  But then technically you could be selecting a non-Prop definition as well…</p>",
        "id": 407821098,
        "sender_full_name": "Jason Rute",
        "timestamp": 1702506252
    },
    {
        "content": "<blockquote>\n<p>The installation seems to indicate that I need to import the project as a Lean dependency. This makes it hard to contribute to Mathlib with this, since I generally just work in the Mathlib repository itself, and I would need to modify Mathlib's lakefile to import this, but then not commit those modifications.</p>\n</blockquote>\n<p>I guess we wouldn't need to worry about that if LeanCopilot could become an official dependency of mathlib master ... I hope it doesn't mean every mathlib user has to download GBs of model weights, otherwise it's probably not too heavy a dependency.</p>\n<p>In the meantime, maybe we could consider making LeanCopilot run in another copy of the mathlib repo (or of whatever project you're working on)? It's probably not very helpful to retrieve the new lemmas that you added in your branch that you want to PR to mathlib, as you know them well and know when to apply them; it's much more useful to retrieve unfamiliar lemmas in the wider existing mathlib; periodically merging master in the copy should be good enough. I don't think much of LeanCopilot's functionalities depend on running within the same project, but correct me if I'm wrong.</p>\n<p>I think most existing LLM code assistants work by looking at the plain text of the current file and other relevant files in your repo, and maybe also communicating with the language server at times. Most are not written in the target language; in fact many support multiple languages. The Lean-native approach of LeanCopilot could have speed advantages, which may not be important to a human user, but may prove crucial if we go ahead to train automated agents, where faster interaction with the environment could lead to faster feedback loop, iteration and improvement, and big savings in resources.</p>",
        "id": 407843995,
        "sender_full_name": "Junyan Xu",
        "timestamp": 1702518164
    },
    {
        "content": "<blockquote>\n<p>making LeanCopilot run in another copy of the mathlib repo</p>\n</blockquote>\n<p>To elaborate, I think a realistic proposal is to split LeanCopilot into a lightweight user-facing client side and a server side. The client just need to implement the <code>suggest_tactics</code>, <code>search_proof</code>, and <code>select_premises</code> (maybe in some generalized extensible form that could support other assistants in the future), and know the server's address to send requests to and receive suggestions from it. It should be relatively easy to get this merged into mathlib master. The server side will host the inference framework, retriever models, indices (are they there yet?), etc. It also needs a copy of mathlib to retrieve from, but that doesn't have to be the client's copy.</p>",
        "id": 407847677,
        "sender_full_name": "Junyan Xu",
        "timestamp": 1702519765
    },
    {
        "content": "<p>But doesn't that ruin the generalizability of the <a href=\"https://github.com/lean-dojo/LeanCopilot#bring-your-own-model\">bring-your-own-model</a> part of the project?  Or is it still possible?  Do you now have to split the custom model into a client and server portion, where the client portion reads the current state and environment, and sends stuff back and forth to the server?</p>",
        "id": 407848884,
        "sender_full_name": "Jason Rute",
        "timestamp": 1702520171
    },
    {
        "content": "<p>And I don't understand the \"also needs a copy of mathlib to retrieve from\".  I'm still not sure if retrieval is intended to be project independent.  Can Kevin retrieve a lemma from his FLT project, or does it only work for lemmas which existed in mathlib when the model was trained (in which case maybe it is possible to have a separate server with its own copy of mathlib since that is all that matters).</p>",
        "id": 407849412,
        "sender_full_name": "Jason Rute",
        "timestamp": 1702520336
    },
    {
        "content": "<p>(Actually, if premise selection is only based on stuff seen during training, then the server doesn't need any access to Mathlib since it was already seen during training.)</p>",
        "id": 407851645,
        "sender_full_name": "Jason Rute",
        "timestamp": 1702521034
    },
    {
        "content": "<p>My experience with retrieval as a user (mostly when I tried <a href=\"https://cursor.sh/\">Cursor</a>) is that you typically need to click a button to refresh the vector index for whatever repository you have open. The speed is actually pretty fast even on a large repository. I could see there being an \"LeanCopilot Server\" VSCode extension that manages this stuff instead of Lean/Mathlib.</p>",
        "id": 407855133,
        "sender_full_name": "llllvvuu",
        "timestamp": 1702522318
    },
    {
        "content": "<p>Decoupled server-client design should make it easier to customize the server side (including the model), no? There should be some protocol that the client and the server both follow, but that's all.</p>\n<p><a href=\"https://leandojo.readthedocs.io/en/latest/user-guide.html#caching\">LeanDojo docs</a> talks about tracing mathlib (or other project) repo and caching the results; they host a number of traced repos on AWS. I'm not sure what the purpose is though; maybe it's just for extraction of training data, because I don't see anything about embedding and storing in a vector database for retrieval.</p>",
        "id": 407855669,
        "sender_full_name": "Junyan Xu",
        "timestamp": 1702522517
    },
    {
        "content": "<p>Sorry, I looked now at the types of custom models it supports and they are all text models, so I think I was thinking of a different project (namely <a href=\"https://github.com/leanprover-community/aesop/blob/master/AesopTest/TacGen.lean\">aesop</a>) that could work with non-text-based models that can directly look at the environment.</p>",
        "id": 407855748,
        "sender_full_name": "Jason Rute",
        "timestamp": 1702522550
    },
    {
        "content": "<p>I know this project depends on aesop, so I guess the main parts are aesop, some llm-step stuff for the interface, and the reprover model in the backend.  And I guess the suggestion is to move the reprover model to a separate server.</p>",
        "id": 407855758,
        "sender_full_name": "Jason Rute",
        "timestamp": 1702522555
    },
    {
        "content": "<p>I know of another (unpublished, but soon to be released) project that works like that.  The ITP client sends a whole bunch of goal and environment data to the server to process.  The server keeps a list of vector embeddings for each definition and also handles predicting new tactics.  The client handles the tree search (which I think it would in this case too since aesop handles the search, but it could go the other way as well where the server does the search as in Lean gym).</p>",
        "id": 407855984,
        "sender_full_name": "Jason Rute",
        "timestamp": 1702522632
    },
    {
        "content": "<p>The aesop integration is <a href=\"https://github.com/leanprover-community/aesop/pull/70\">this PR</a>. I think it may well serve as part of the client side.</p>",
        "id": 407856200,
        "sender_full_name": "Junyan Xu",
        "timestamp": 1702522719
    },
    {
        "content": "<p>I'm surprised there is still interface code from llm-step. We upstreamed the <code>Try these</code> widget to Std already, but perhaps LeanCopilot is not using that.</p>",
        "id": 407856335,
        "sender_full_name": "Scott Morrison",
        "timestamp": 1702522784
    },
    {
        "content": "<p>The word \"embedding\" appears twice in the <a href=\"https://arxiv.org/pdf/2306.15626.pdf\">LeanDojo paper</a>, and they do say it can be precomputed. I agree it's not very useful to keep the mathlib repo around once you have extracted the cache including embeddings.</p>\n<p><a href=\"/user_uploads/3121/kCjEWpbGz70ryc6egfXMtvWl/image.png\">image.png</a></p>\n<div class=\"message_inline_image\"><a href=\"/user_uploads/3121/kCjEWpbGz70ryc6egfXMtvWl/image.png\" title=\"image.png\"><img src=\"/user_uploads/3121/kCjEWpbGz70ryc6egfXMtvWl/image.png\"></a></div>",
        "id": 407857659,
        "sender_full_name": "Junyan Xu",
        "timestamp": 1702523174
    },
    {
        "content": "<p>Even if it is not precomputed, you can just still use the client to send the environment data to the server to compute lemma embeddings or to align with pre-computed embeddings.  (At least that is what the other project I know about does.)</p>",
        "id": 407858166,
        "sender_full_name": "Jason Rute",
        "timestamp": 1702523371
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"584504\">Kaiyu Yang</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/LeanCopilot/near/407793919\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"389019\">Chris Birkbeck</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/LeanCopilot/near/407216529\">said</a>:</p>\n<blockquote>\n<p>This is probably a stupid question, but how far are we from being able to use something like this to \"clean up\" some code and it get PR ready or close to PR ready? I have a bunch of stuff in a repo I need to PR and while I don't expect something like this to be able to prove the results it contains, it would be nice if it could learn from the proofs that are in the repo and then just tidy it up (i.e.  I say this is the main result I want you to prove, then go, find the proof in the repo, break up into smaller results, remove unused results, fix spacing/layout etc).  Perhaps I'm just way behind and this is already possible, or maybe  this is sci fi and its actually hard to do and we are not there yet.</p>\n</blockquote>\n<p>We think it would be interesting to have something that can learn from your repo (as long as it's public on GitHub and can be built via <code>lake build</code>), automatically fill in <code>sorry</code> and submit pull requests to your repo. I don't see a fundamental difficulty here. It's just that we have limited personnel and have to priortize.</p>\n<p>BTW, I was wondering if \"PR\" here means pull requests or public relations <span aria-label=\"rolling on the floor laughing\" class=\"emoji emoji-1f923\" role=\"img\" title=\"rolling on the floor laughing\">:rolling_on_the_floor_laughing:</span>.</p>\n</blockquote>\n<p>Sorry , yes PR was Pull request, to mathlib in this case. Its encouraging that such a thing could maybe be done soon. One of the things that worries me is that there are lots of bits of code out there in repos that people  (including me) haven't had the time to add to mathlib and slowly gets lost. Something like this, even if its not perfect, would really help.</p>",
        "id": 407942374,
        "sender_full_name": "Chris Birkbeck",
        "timestamp": 1702554224
    },
    {
        "content": "<p>OK so if I want to try this out, is it still the case that I can't use it in a project which depends on mathlib master?</p>",
        "id": 408026356,
        "sender_full_name": "Kevin Buzzard",
        "timestamp": 1702581019
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"110087\">Scott Morrison</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/LeanCopilot/near/407856335\">said</a>:</p>\n<blockquote>\n<p>I'm surprised there is still interface code from llm-step. We upstreamed the <code>Try these</code> widget to Std already, but perhaps LeanCopilot is not using that.</p>\n</blockquote>\n<p>I'm not aware of it. Can you give me a pointer? Thx!</p>",
        "id": 408036179,
        "sender_full_name": "Kaiyu Yang",
        "timestamp": 1702583969
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"110038\">Kevin Buzzard</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/LeanCopilot/near/408026356\">said</a>:</p>\n<blockquote>\n<p>OK so if I want to try this out, is it still the case that I can't use it in a project which depends on mathlib master?</p>\n</blockquote>\n<p>Lean Copilot only depends on aesop, so you can definitely use it in mathlib or other repos depending on mathlib. <a href=\"https://github.com/yangky11/miniF2F-lean4\">Here</a> is an example of a repo depending on both mathlib and Lean Copilot.</p>",
        "id": 408036430,
        "sender_full_name": "Kaiyu Yang",
        "timestamp": 1702584024
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"584504\">Kaiyu Yang</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/LeanCopilot/near/408036179\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"110087\">Scott Morrison</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/LeanCopilot/near/407856335\">said</a>:</p>\n<p>I'm not aware of it. Can you give me a pointer? Thx!</p>\n</blockquote>\n<p>(Sorry, my fault for not updating you on this!)</p>\n<p>See <a href=\"https://github.com/leanprover/std4/blob/main/Std/Tactic/TryThis.lean#L386\">https://github.com/leanprover/std4/blob/main/Std/Tactic/TryThis.lean#L386</a> for the main entry point <code>addSuggestions</code>. This support multiple suggestions, and has customisation hooks for indicating tactics that do or don't succeed.</p>\n<p>Also useful is the <code>suggestion</code> function provided by Mathlib's <code>hint</code> tactic, that takes a piece of syntax representing a tactic invocation, and takes care of running it against the goal, checking if it succeeded, and preparing the <code>Suggestion</code> with appropriate formatting that can be passed to the <code>TryThis</code> widget.</p>\n<p>(To be clear, this is all inspired by llm-step's implementation, just with a bit more engineering work. :-)</p>\n<p>It would be great if this can all be reused.</p>\n<p>(The components that are still in Mathlib have open PRs moving them to Std; if it's helpful let me know and I can hurry those along. :-)</p>",
        "id": 408062701,
        "sender_full_name": "Scott Morrison",
        "timestamp": 1702595291
    },
    {
        "content": "<p>Thanks <span class=\"user-mention\" data-user-id=\"110087\">@Scott Morrison</span> ! It's great to see these common frontend utilities are moving to the upstream. We'll re-use it. In addition, do you have a similar function for displaying retrieved premises? For each premise, we have</p>\n<ul>\n<li>fully qualified name</li>\n<li>which module (file) it comes from</li>\n<li>the code for defining it (optional) </li>\n</ul>\n<p>Currently, we simply use the fully qualified name to find its type and doc-string (if available) and use <code>logInfo</code> to print it to the infoview panel. However, it would be great if it can be more interactive, e.g., displaying additional information when the user hovers the mouse over it (similar to hovering over the goals in the infoview panel), or allowing the user to \"go-to definition\". Would that also be useful for the hammer Lean FRO is developing?</p>",
        "id": 408363803,
        "sender_full_name": "Kaiyu Yang",
        "timestamp": 1702749345
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"115715\">Jason Rute</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/LeanCopilot/near/407821098\">said</a>:</p>\n<blockquote>\n<p>But then technically you could be selecting a non-Prop definition as well…</p>\n</blockquote>\n<p><span class=\"user-mention\" data-user-id=\"110049\">@Mario Carneiro</span>  That was the main reason we used the term \"premise\" instead of \"lemma\", though informally (e.g., when explaining it to people coming to our poster), I actually use \"lemma\".</p>",
        "id": 408364126,
        "sender_full_name": "Kaiyu Yang",
        "timestamp": 1702749614
    },
    {
        "content": "<p>I'm not sure that makes it better, people don't call definitions premises either</p>",
        "id": 408364199,
        "sender_full_name": "Mario Carneiro",
        "timestamp": 1702749664
    },
    {
        "content": "<p>I guess \"constant\" is technically the right term for Lean? But people would be confused.</p>",
        "id": 408364281,
        "sender_full_name": "Kaiyu Yang",
        "timestamp": 1702749718
    },
    {
        "content": "<p>How about we just say \"lemma suggestions\"?</p>",
        "id": 408388206,
        "sender_full_name": "Scott Morrison",
        "timestamp": 1702773038
    },
    {
        "content": "<p>We do not have a function for displaying \"lemma suggestions\", but creating one sounds like a great idea. As <span class=\"user-mention\" data-user-id=\"548935\">@Thomas Murrills</span> did such a great job with the <code>Try these</code> widget I wonder if they might be interested? :-)</p>",
        "id": 408388375,
        "sender_full_name": "Scott Morrison",
        "timestamp": 1702773128
    },
    {
        "content": "<p>Thanks for thinking of me! You caught me at a good time; I’ve got a few days free! :D I’ll be happy to give it a shot! First I want to think a bit more deeply about the actual use cases of such a widget, and try to understand what information will (or might be) relevant to the user in these cases. I’ll probably ask some questions here as I think through it! :)</p>\n<p>(I also want to consider whether it would make sense to integrate this with <code>Try these</code> or have a standalone widget. The answer might be readily apparent once I think about it…)</p>",
        "id": 408395411,
        "sender_full_name": "Thomas Murrills",
        "timestamp": 1702777517
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"584504\">@Kaiyu Yang</span> </p>\n<p>I recently came across the <code>mistral-7b</code> model and was surprised by its capability. I ran some initial experiments, including using it to detect whether texts are generated AI model. I found it efficient fine-tuning (fully trained on a TPU in just 2 hours with Lora layers) and impressive performance (0.85 ROC accuracy). </p>\n<p>Has anyone explored using <code>mistral-7b</code> on Leandrojo? That would be an very interesting experiment. <span aria-label=\"grinning face with smiling eyes\" class=\"emoji emoji-1f601\" role=\"img\" title=\"grinning face with smiling eyes\">:grinning_face_with_smiling_eyes:</span></p>",
        "id": 408410967,
        "sender_full_name": "Min-Hsien Weng",
        "timestamp": 1702791105
    },
    {
        "content": "<p>I'm currently trying to install this, and getting the error <code>error: unknown executable </code>«LeanCopilot/download»`; has anyone else come across this?</p>",
        "id": 408451912,
        "sender_full_name": "Eric Rodriguez",
        "timestamp": 1702828488
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"284160\">Eric Rodriguez</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/LeanCopilot/near/408451912\">said</a>:</p>\n<blockquote>\n<p>I'm currently trying to install this, and getting the error <code>error: unknown executable \n</code>«LeanCopilot/download»`; has anyone else come across this?</p>\n</blockquote>\n<p>Maybe open an issue on GitHub?</p>",
        "id": 408454347,
        "sender_full_name": "Kaiyu Yang",
        "timestamp": 1702830629
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"638383\">Min-Hsien Weng</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/LeanCopilot/near/408410967\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"584504\">Kaiyu Yang</span> </p>\n<p>I recently came across the <code>mistral-7b</code> model and was surprised by its capability. I ran some initial experiments, including using it to detect whether texts are generated AI model. I found it efficient fine-tuning (fully trained on a TPU in just 2 hours with Lora layers) and impressive performance (0.85 ROC accuracy). </p>\n<p>Has anyone explored using <code>mistral-7b</code> on Leandrojo? That would be an very interesting experiment. <span aria-label=\"grinning face with smiling eyes\" class=\"emoji emoji-1f601\" role=\"img\" title=\"grinning face with smiling eyes\">:grinning_face_with_smiling_eyes:</span></p>\n</blockquote>\n<p>This is something I have always wanted to do but haven't had the chance to do yet :). I probably won't do it in the very near future, but I am interested to see others do it.</p>",
        "id": 408746475,
        "sender_full_name": "Kaiyu Yang",
        "timestamp": 1702956771
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"548935\">Thomas Murrills</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/LeanCopilot/near/408395411\">said</a>:</p>\n<blockquote>\n<p>Thanks for thinking of me! You caught me at a good time; I’ve got a few days free! :D I’ll be happy to give it a shot! First I want to think a bit more deeply about the actual use cases of such a widget, and try to understand what information will (or might be) relevant to the user in these cases. I’ll probably ask some questions here as I think through it! :)</p>\n<p>(I also want to consider whether it would make sense to integrate this with <code>Try these</code> or have a standalone widget. The answer might be readily apparent once I think about it…)</p>\n</blockquote>\n<p>Thank you! I'm happy to discuss more details when you set out to implement it!</p>",
        "id": 408747310,
        "sender_full_name": "Kaiyu Yang",
        "timestamp": 1702956910
    },
    {
        "content": "<p>It seems to me that the authors forgot to upload the initially unavailable (because their mathlib4 digestion scripts weren't able to dump premise details like they did for Lean3 and I assume used for the RAG fine-tuning) Lean4 RAG model, see it's missing: <a href=\"https://github.com/lean-dojo/ReProver?tab=readme-ov-file#using-trained-models-on-hugging-face\">https://github.com/lean-dojo/ReProver?tab=readme-ov-file#using-trained-models-on-hugging-face</a> (I checked the hugging face account they uploaded under, it's not there, and it wasn't part of v1 of their ArXiv submission, either).</p>\n<p>Not that goal-conditioned auto-complete for tactics isn't already very useful, but there's a reason they went through the effort to set up a premise retrieval engine (the paper shows aesop (iirc best-first) proof search on top of the success-probability-annotated tactic suggestions (with RAG) to be state-of-the-art in auto-provers for Lean3, and they didn't change anything substantial to Lean4)...</p>",
        "id": 411526577,
        "sender_full_name": "namibj",
        "timestamp": 1704565139
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"584504\">@Kaiyu Yang</span> and maybe this should be moved to the other <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/LeanCopilot/near/407175937\">thread</a>.</p>",
        "id": 411535581,
        "sender_full_name": "Junyan Xu",
        "timestamp": 1704572122
    },
    {
        "content": "<p>Oh, thanks; how would/should I go about said move?</p>",
        "id": 411535651,
        "sender_full_name": "namibj",
        "timestamp": 1704572175
    },
    {
        "content": "<p>would need an administrator to move across streams, I think ...</p>",
        "id": 411535917,
        "sender_full_name": "Junyan Xu",
        "timestamp": 1704572438
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"584504\">Kaiyu Yang</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/LeanCopilot/near/407175937\">said</a>:</p>\n<blockquote>\n<ul>\n<li>Retrieval for premise selection</li>\n</ul>\n</blockquote>\n<p>I <a href=\"#narrow/stream/113488-general/topic/LeanCopilot/near/411526577\">believe I'm missing/you forgot to upload</a> the fine-tuned RAG model for Lean4?</p>\n<p>Also CTranslate2 either crashes on GPU attempt or refuses to build for me (at least from your CMake setup), so I've settled for a bit on the non-RAG generator running on my 5950X via OpenBLAS as it's sufficiently interactive for now.</p>\n<p>I'd maybe try to get a half-remote (so you can time-share the GPU with other GPU users/usage) autocomplete provider based on an (opportunistically) stateful prompt-completing beam search decoder: KV cache the encoder results, re-steer beam search to respond to latest typed prefix, feed suggestions to the usual cursor-localized drop-down of regular autocomplete.</p>\n<p>I'd have to try the RAG generator first, though, and hear feedback from others more familiar with Lean about it's casual usefulness.</p>",
        "id": 411536988,
        "sender_full_name": "namibj",
        "timestamp": 1704573648
    },
    {
        "content": "<p>4 messages were moved here from <a class=\"stream-topic\" data-stream-id=\"113488\" href=\"/#narrow/stream/113488-general/topic/LeanCopilot\">#general &gt; LeanCopilot</a> by <span class=\"user-mention silent\" data-user-id=\"110087\">Scott Morrison</span>.</p>",
        "id": 411544137,
        "sender_full_name": "Notification Bot",
        "timestamp": 1704581016
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"678785\">@namibj</span> We're in the process of re-training a new version of the models for NeurIPS's Jan 15 deadline for final updates to the cam-ready version. We will update the ReProver repo with those models (including the Lean 4 w/ retrieval model). Feel free to DM me in case I forgot.</p>\n<p>For CTranslate2 or other problems with Lean Copilot, please open an issue in the LeanCopilot repo.</p>",
        "id": 411808009,
        "sender_full_name": "Kaiyu Yang",
        "timestamp": 1704744423
    },
    {
        "content": "<p>I'll ping you when they're not on HF or mentioned in the repo when I look on the 17th.<br>\nThanks for the info!</p>",
        "id": 411808279,
        "sender_full_name": "namibj",
        "timestamp": 1704744545
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"584504\">@Kaiyu Yang</span> It is the 17th, and the updates are neither on the ReProver repo nor are there updated models on your <span aria-label=\"hug\" class=\"emoji emoji-1f917\" role=\"img\" title=\"hug\">:hug:</span> page.</p>",
        "id": 415969937,
        "sender_full_name": "namibj",
        "timestamp": 1705477952
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"678785\">namibj</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/LeanCopilot/near/415969937\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"584504\">Kaiyu Yang</span> It is the 17th, and the updates are neither on the ReProver repo nor are there updated models on your <span aria-label=\"hug\" class=\"emoji emoji-1f917\" role=\"img\" title=\"hug\">:hug:</span> page.</p>\n</blockquote>\n<p>Updated!</p>",
        "id": 416332593,
        "sender_full_name": "Kaiyu Yang",
        "timestamp": 1705524071
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"584504\">Kaiyu Yang</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/LeanCopilot/near/416332593\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"678785\">namibj</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/LeanCopilot/near/415969937\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"584504\">Kaiyu Yang</span> It is the 17th, and the updates are neither on the ReProver repo nor are there updated models on your <span aria-label=\"hug\" class=\"emoji emoji-1f917\" role=\"img\" title=\"hug\">:hug:</span> page.</p>\n</blockquote>\n<p>Updated!</p>\n</blockquote>\n<p>Will LeanCopilot get the upgrade soon, or should I attempt to take care of it?</p>",
        "id": 416452965,
        "sender_full_name": "namibj",
        "timestamp": 1705528924
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"678785\">@namibj</span> Currently Lean Copilot still uses the previous models. We have other plans for keeping Lean Copilot's models up to date (potentially better than the models in the LeanDojo paper). I'll send an update later.</p>",
        "id": 416624122,
        "sender_full_name": "Kaiyu Yang",
        "timestamp": 1705601231
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"584504\">Kaiyu Yang</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/LeanCopilot/near/416624122\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"678785\">namibj</span> Currently Lean Copilot still uses the previous models. We have other plans for keeping Lean Copilot's models up to date (potentially better than the models in the LeanDojo paper). I'll send an update later.</p>\n</blockquote>\n<p>If you don't expect this to be wasted work, I'd like to go ahead and try my best to unlock RAG in LeanCopilot, assuming I can get the <del>retriever to work at all (currently it complains because the CTranslate2 CUDA support of the prebuild LeanCopilot errors out on my system (I blame my old GTX 970); forcing CUDA doesn't seem to work for the retriever, just for the generator, which is how I used it for the time being), but I was able to build it without CUDA on my Laptop at least).</del> <strong>Edit: Reading code made me realize how I can force CUDA off for the <code>encode</code> function, and thus the <code>select_premises</code> tactic.</strong></p>\n<p>I'll assume the official guide to convert the model to CTranslate2 would work, or there's code/documentation on how the current CTranslate2 model versions were created.</p>\n<p>Worst-case, I'd attempt to use the ReProver repo as a base to cook up a local PyTorch-based RAG for use via the external model feature of LeanCopilot.</p>\n<p>On a side note, while preparing to allow manually turning CUDA usage off, I noticed that the <code>retrieve</code> function in <code>ct2.cpp</code> lacks CUDA synchronization, which fits the theory that the crash happens from accessing unfinished/uninitialized output of the TopK operator (assuming the MatMul -&gt; TopK sequence at least executes in a sequentially consistent manner).</p>\n<p>If you don't make me hold off on this \"upgrade LeanCopilot to RAG\", I'd see to tackle that bug together with getting \"current project\"(and/or \"in scope\") premises included in the retrieval.</p>",
        "id": 418079345,
        "sender_full_name": "namibj",
        "timestamp": 1706189355
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"678785\">@namibj</span> </p>\n<blockquote>\n<p>there's code/documentation on how the current CTranslate2 model versions were created.</p>\n</blockquote>\n<p>Please see the README of the CTranslate2 models on my Hugging Face.</p>\n<blockquote>\n<p>Worst-case, I'd attempt to use the ReProver repo as a base to cook up a local PyTorch-based RAG for use via the external model feature of LeanCopilot.</p>\n</blockquote>\n<p>I think that is a good start. You're less likely to run into problems when running the model in Python.</p>\n<blockquote>\n<p>On a side note, while preparing to allow manually turning CUDA usage off, I noticed that the retrieve function in ct2.cpp lacks CUDA synchronization, which fits the theory that the crash happens from accessing unfinished/uninitialized output of the TopK operator (assuming the MatMul -&gt; TopK sequence at least executes in a sequentially consistent manner).</p>\n</blockquote>\n<p>That could be the reason for some CUDA errors we encountered when using the retrieval. Could you please open an issue to elaborate on that in LeanCopilot's GitHub repo? Thanks!</p>\n<p>For incorporating retrieval into LeanCopilot, we also need a way to query and index the corpus of premises. Currently, the <code>select_premises</code> tactic in LeanCopilot only retrieves from a fixed snapshot of mathlib. Ideally, we want to go beyond that and make the corpus dynamic, though I'm not sure what's the best way to do that.</p>",
        "id": 418199776,
        "sender_full_name": "Kaiyu Yang",
        "timestamp": 1706238887
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"584504\">Kaiyu Yang</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/LeanCopilot/near/418199776\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"678785\">namibj</span> </p>\n<blockquote>\n<p>there's code/documentation on how the current CTranslate2 model versions were created.</p>\n</blockquote>\n<p>Please see the README of the CTranslate2 models on my Hugging Face.</p>\n</blockquote>\n<p>Ahh, yeah, that's about what I remembered; perfect then.</p>\n<blockquote>\n<blockquote>\n<p>Worst-case, I'd attempt to use the ReProver repo as a base to cook up a local PyTorch-based RAG for use via the external model feature of LeanCopilot.</p>\n</blockquote>\n<p>I think that is a good start. You're less likely to run into problems when running the model in Python.</p>\n</blockquote>\n<p>It _is_ working, though, besides one reproducible crash in <code>init_premise_embeddings</code> on the second <code>select_premises</code> tactic; I hope to have gotten a pointer to how to <code>rr record</code> it without also recording VSCode itself; but if still unclear, I'll just attempt the latter tomorrow (ETA about 25 hours from now).</p>\n<blockquote>\n<blockquote>\n<p>On a side note, while preparing to allow manually turning CUDA usage off, I noticed that the retrieve function in ct2.cpp lacks CUDA synchronization, which fits the theory that the crash happens from accessing unfinished/uninitialized output of the TopK operator (assuming the MatMul -&gt; TopK sequence at least executes in a sequentially consistent manner).</p>\n</blockquote>\n<p>That could be the reason for some CUDA errors we encountered when using the retrieval. Could you please open an issue to elaborate on that in LeanCopilot's GitHub repo? Thanks!</p>\n</blockquote>\n<p>I'm actually planning to 1-up this and submit a PR directly; I expect to need to rework the function anyways to include non-mathlib premises in the initial ranking part of the RAG, so I might as well attempt to debug/fix this after learning enough of the CTranslate2 C++ API.</p>\n<p>Either way, that operation doesn't need to run on GPU; the encode step is far more expensive.</p>\n<blockquote>\n<p>For incorporating retrieval into LeanCopilot, we also need a way to query and index the corpus of premises. Currently, the <code>select_premises</code> tactic in LeanCopilot only retrieves from a fixed snapshot of mathlib. Ideally, we want to go beyond that and make the corpus dynamic, though I'm not sure what's the best way to do that.</p>\n</blockquote>\n<p>I'm aware of what it currently does; ideas I have include upstreaming a hook suitable for dumping premises in the required format during a <code>lake build</code>, as well as more-or-less \"just\" digesting the local <code>.olean</code> files to understand what's locally relevant in terms of premises.<br>\nI'll ask for how to do it in a way that probably won't break with, like, lean 4.7.0 (~10 weeks from now) (nothing special about that version. Just a hypothetical issue.)</p>\n<p>If the premise dumping can run during a <code>lake build</code>, we could probably ask Reservoir to collect it across the ecosystem on the next build of each package.</p>",
        "id": 418208166,
        "sender_full_name": "namibj",
        "timestamp": 1706241387
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"678785\">@namibj</span> <span class=\"user-mention\" data-user-id=\"584504\">@Kaiyu Yang</span>  In case it helps, here is roughly how we do this in Graph2Tac.  Graph2Tac stores vector embeddings for each definition (including theorems) in the global context.  These are used for both argument selection (similar to premise selection) and for node embeddings in the graph model.   The embeddings are stored inside the neural network weights, and there are roughly two categories of embeddings (both stored in the same table)</p>\n<ul>\n<li>embeddings of definitions seen during training which already come stored in the network weights</li>\n<li>embeddings of new definitions which are updated in the network weights in real time</li>\n</ul>",
        "id": 418306478,
        "sender_full_name": "Jason Rute",
        "timestamp": 1706285463
    },
    {
        "content": "<p>When <code>synth</code> or <code>Suggest</code> is run by the user, then Tactician API sends to the Python server all the definitions in the global environment that haven't already been sent over.  If we have Tactician's auto-caching on, then Tactician API also sends over definitions after every new definition.  This amortizes the computation of new embeddings, so the user doesn't notice any delay.  We also have it set up that if the user backs up in the context, then Tactician knows this as well, and it is reflected in the information Tactician sends to the Python client (because, again, Tactician only sends updates to context information, not the full context).</p>",
        "id": 418306500,
        "sender_full_name": "Jason Rute",
        "timestamp": 1706285468
    },
    {
        "content": "<p>Now when the Python server gets a new list of definitions, it checks the hashes of these definitions against the hashes for all embeddings it currently has in its table.  (The hashes are based on the graph of the definition and its name, so they are invariant between different instances of running Coq.)  If a definition hash is new, it calculates the embedding for that definition using the neural network and adds it to the next available spot in the embedding table.  It also associates that hash with the index in the embedding table.  Since the model comes already with embeddings for definitions seen during training, including all definitions in the standard library, it doesn't have to recalculate those.   (As an implementation detail, we keep the table larger than the number of currently embedded definitions, so we can add new definitions without having to resize the array and recompile our model, except when we run out of room.  This is like a resizable array.)  (As another implementation detail, we don't delete embeddings, so there could be many embeddings seen during training which are not used as well as embeddings for definitions which the user wrote but then backtracked and edited.  This isn't a big deal since we only do argument selection over the definitions available in the current global context.)</p>",
        "id": 418306518,
        "sender_full_name": "Jason Rute",
        "timestamp": 1706285474
    },
    {
        "content": "<p>I don't know how much of this is easily implementable in Lean, and it helps in our case that calculating the embeddings for a few definitions is really fast in our model.  (But still, importing a large package can take a number of seconds to build the table for those definitions.)</p>",
        "id": 418306538,
        "sender_full_name": "Jason Rute",
        "timestamp": 1706285479
    },
    {
        "content": "<p>My coauthors <span class=\"user-mention\" data-user-id=\"306713\">@Lasse Blaauwbroek</span> and <span class=\"user-mention\" data-user-id=\"133339\">@Mirek Olšák</span> were more involved in some of these technical details and have more to add, but I also want to shout out to them out for making a system which not only works in theory, but works in practice.</p>",
        "id": 418306635,
        "sender_full_name": "Jason Rute",
        "timestamp": 1706285506
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"115715\">Jason Rute</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/LeanCopilot/near/418306478\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"678785\">namibj</span> <span class=\"user-mention silent\" data-user-id=\"584504\">Kaiyu Yang</span>  In case it helps, here is roughly how we do this in Graph2Tac.  Graph2Tac stores vector embeddings for each definition (including theorems) in the global context.  These are used for both argument selection (similar to premise selection) and for node embeddings in the graph model.   The embeddings are stored inside the neural network weights, and there are roughly two categories of embeddings (both stored in the same table)</p>\n<ul>\n<li>embeddings of definitions seen during training which already come stored in the network weights</li>\n<li>embeddings of new definitions which are updated in the network weights in real time</li>\n</ul>\n</blockquote>\n<p>I mean, yeah, I expect something like that. The question is more of how to integrate with Lean/Lake the build system/compiler, than how to handle the entries once they made their way to the vector database/retriever/encoder (\"RAG machinery\").</p>",
        "id": 418307090,
        "sender_full_name": "namibj",
        "timestamp": 1706285643
    },
    {
        "content": "<p>I would think the best approach, but also the most engineering is to have a three entry points:</p>\n<ol>\n<li>A model comes stored with embeddings for core, std, and mathlib.</li>\n<li>During local compilation, the embedding table is updated in the local model.</li>\n<li>During user interaction, the embedding table is updated in real time by communicating with Lean incremental information about changes in the global context.</li>\n</ol>\n<p>My point is mainly that 1. and 3. are possible, and we have experience doing it in Coq if you want to ask questions.</p>",
        "id": 418308574,
        "sender_full_name": "Jason Rute",
        "timestamp": 1706286128
    },
    {
        "content": "<p>(But I don't mean to presume we have all the answers, and you are correct that working with Lean/Lake may be a very different that our work in Coq.)</p>",
        "id": 418309185,
        "sender_full_name": "Jason Rute",
        "timestamp": 1706286336
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"115715\">Jason Rute</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/LeanCopilot/near/418308574\">said</a>:</p>\n<blockquote>\n<p>I would think the best approach, but also the most engineering is to have a three entry points:</p>\n<ol>\n<li>A model comes stored with embeddings for core, std, and mathlib.</li>\n<li>During local compilation, the embedding table is updated in the local model.</li>\n<li>During user interaction, the embedding table is updated in real time by communicating with Lean about incremental information about changes in the global context.</li>\n</ol>\n<p>My point is mainly that 1. and 3. are possible, and we have experience doing it in Coq if you want to ask questions.</p>\n</blockquote>\n<p>Well, those things ought to happen, maybe not quite structured like this.<br>\nThe question is where and how exactly 2 shall happen.</p>\n<p>I hope to get to this the weekend/early next week; most of next week is blocked off in my schedule though.<br>\nAs I understand it, embedding expense would be roughly in the 10's of GFLOPs for common single-line theorems.<br>\nI.e., it's comparatively really expensive, so aggressive caching is critical. I just haven't figured out yet where and how to best do that. Especially as there is some mild pretty-printing/normalization going on for symbol resolution as I understand it, that might not be trivial in all convenient ways where these new definitions could be treated.</p>",
        "id": 418310052,
        "sender_full_name": "namibj",
        "timestamp": 1706286537
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"678785\">@namibj</span> That sounds great. Please keep me posted. I think making this work for Lean is meaningful not only for RAG but also for selecting useful lemmas for humans.</p>",
        "id": 418552428,
        "sender_full_name": "Kaiyu Yang",
        "timestamp": 1706492058
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"584504\">Kaiyu Yang</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/LeanCopilot/near/418552428\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"678785\">namibj</span> That sounds great. Please keep me posted. I think making this work for Lean is meaningful not only for RAG but also for selecting useful lemmas for humans.</p>\n</blockquote>\n<p>Yeah; though IMO the RAG aspect should help humans interpret how those premises could be applied to the current goal/state.</p>\n<p>Furthermore, may I request you upload the ct2 conversion of <code>kaiyuy/leandojo-lean4-retriever-tacgen-byt5-small</code>? It's clearer/easier from the licensing POV and would keep the commits to LeanCopilot from containing someone else's HF repo URLs.</p>\n<p>Also, can you clarify the input spec for the Lean4 RAG generator model? I.e., does it need that <code>&lt;a&gt;&lt;/a&gt;</code> wrapping for the premise names that the Lean3 RAG generator model seems to need?</p>\n<p>Futhermore, to my understanding larger transformers are more efficient to train to some particular level of quality, but rather more expensive to run inference on: have you run any ablation studies whatsoever with a larger ByT5 than the smallest that is widely used in the paper?<br>\nDo the 100 hours/5 days of A100 training time for the fine tuning of <code>google/byt5-small</code> refer to single-model, or do they cover both retriever and generator?</p>",
        "id": 418698050,
        "sender_full_name": "namibj",
        "timestamp": 1706553697
    },
    {
        "content": "<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"678785\">namibj</span> <br>\nFuthermore, to my understanding larger transformers are more efficient to train to some particular level of quality, but rather more expensive to run inference on: have you run any ablation studies whatsoever with a larger ByT5 than the smallest that is widely used in the paper?</p>\n</blockquote>\n<p>I came across a python library <code>vLLM</code> that can speed up the inference time of large language models. It's reported that it can generate the texts over 1000 tokens/second using the Mistral-7b model with 128 parallel requests.</p>\n<p>Currently, vLLM supports 20 models, but ByT5 isn't yet included. However, you can submit a request for its integration, and I believe they are happy to incorporate Google's ByT5 model.</p>\n<p>vLLM: <a href=\"https://github.com/vllm-project/vllm\">https://github.com/vllm-project/vllm</a></p>",
        "id": 419341293,
        "sender_full_name": "Min-Hsien Weng",
        "timestamp": 1706825026
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"638383\">Min-Hsien Weng</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/LeanCopilot/near/419341293\">said</a>:</p>\n<blockquote>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"678785\">namibj</span> <br>\nFuthermore, to my understanding larger transformers are more efficient to train to some particular level of quality, but rather more expensive to run inference on: have you run any ablation studies whatsoever with a larger ByT5 than the smallest that is widely used in the paper?</p>\n</blockquote>\n<p>I came across a python library <code>vLLM</code> that can speed up the inference time of large language models. It's reported that it can generate the texts over 1000 tokens/second using the Mistral-7b model with 128 parallel requests.</p>\n<p>Currently, vLLM supports 20 models, but ByT5 isn't yet included. However, you can submit a request for its integration, and I believe they are happy to incorporate Google's ByT5 model.</p>\n<p>vLLM: <a href=\"https://github.com/vllm-project/vllm\">https://github.com/vllm-project/vllm</a></p>\n</blockquote>\n<p>Any model similar enough to the listed ones and on Huggingface is automatically supported. For example, I run <em>MorphProver</em> using vLLM following the same instructions as Mistral.</p>\n<p>I don't know if this applies to <code>ByT5</code> though.</p>",
        "id": 419379410,
        "sender_full_name": "Siddhartha Gadgil",
        "timestamp": 1706845901
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"638383\">@Min-Hsien Weng</span> <span class=\"user-mention\" data-user-id=\"130575\">@Siddharth Bhat</span> vLLm currently does not support encoder-decoder models, though I believe there is a pull request working on that. For decoder-only models, running it using vLLM may help significantly in speed.</p>",
        "id": 419473723,
        "sender_full_name": "Kaiyu Yang",
        "timestamp": 1706885854
    },
    {
        "content": "<p>With beam decoding multi-request processing on ByT5 is not that high-impact.</p>",
        "id": 419473963,
        "sender_full_name": "namibj",
        "timestamp": 1706885919
    },
    {
        "content": "<p>1) 2/3rds effort is the encoder. 2) output is shorter than input. 3) you run output token processing in parallel through the beam decoding anyways.</p>",
        "id": 419474464,
        "sender_full_name": "namibj",
        "timestamp": 1706886047
    },
    {
        "content": "<blockquote>\n<p>Furthermore, may I request you upload the ct2 conversion of kaiyuy/leandojo-lean4-retriever-tacgen-byt5-small</p>\n</blockquote>\n<p>Uploading. Will be ready soon.</p>\n<blockquote>\n<p>does it need that &lt;a&gt;&lt;/a&gt; wrapping</p>\n</blockquote>\n<p>Yes. For details see <a href=\"https://github.com/lean-dojo/ReProver/blob/f705b0b4a02ce780929a8d19c8270e9fdddeab9f/common.py#L391\"><code>format_augmented_state</code></a> (with <code>p_drop == 0</code>)</p>\n<blockquote>\n<p>have you run any ablation studies whatsoever with a larger ByT5 than the smallest that is widely used in the paper?</p>\n</blockquote>\n<p>We tried google/byt5-base but the result was similar to google/byt5-small. There are other people currently trying Mistral 7B or CodeLLaMA: <a href=\"https://github.com/lean-dojo/LeanDojo/discussions/5\">https://github.com/lean-dojo/LeanDojo/discussions/5</a>.</p>\n<blockquote>\n<p>Do the 100 hours/5 days of A100 training time for the fine tuning of google/byt5-small refer to single-model, or do they cover both retriever and generator?</p>\n</blockquote>\n<p>That is retriever + generator. Training the generator needs ~3 days.</p>",
        "id": 419475743,
        "sender_full_name": "Kaiyu Yang",
        "timestamp": 1706886444
    },
    {
        "content": "<p>Thanks!</p>",
        "id": 419475912,
        "sender_full_name": "namibj",
        "timestamp": 1706886500
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"584504\">@Kaiyu Yang</span> I'm trying to run Lean Copilot's <code>suggest_tactics</code> (or <code>search_proof</code>) on a compute cluster via <code>lake build</code>.  I'm getting the following error:</p>\n<div class=\"codehilite\" data-code-language=\"Text only\"><pre><span></span><code>error: stderr:\nterminate called after throwing an instance of 'std::runtime_error'\nterminate called recursively\nerror: external command `/u/jasonrute/.elan/toolchains/leanprover--lean4---v4.5.0-rc1/bin/lean` exited with code 134\n</code></pre></div>\n<p>Do you have any idea what could be causing this and how to fix it?  (It works fine on my laptop.)</p>",
        "id": 420474881,
        "sender_full_name": "Jason Rute",
        "timestamp": 1707404106
    },
    {
        "content": "<p>Replacing <code>suggest_tactics</code> with <code>aesop?</code> doesn't give this error, so it is likely a lean copilot thing.</p>",
        "id": 420475187,
        "sender_full_name": "Jason Rute",
        "timestamp": 1707404194
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"115715\">@Jason Rute</span> Did you try restarting the file? Did it fix the error?</p>",
        "id": 420476162,
        "sender_full_name": "Kaiyu Yang",
        "timestamp": 1707404453
    },
    {
        "content": "<p>I'm using <code>lake build</code> from the command line to compile the project (including the file which has <code>suggest_tactics</code> in it), so I don't think \"restarting the file\" applies here.</p>",
        "id": 420476470,
        "sender_full_name": "Jason Rute",
        "timestamp": 1707404528
    },
    {
        "content": "<p>I assume something is broken with running the model.  Is there a simple way to test if that works?</p>",
        "id": 420476776,
        "sender_full_name": "Jason Rute",
        "timestamp": 1707404615
    },
    {
        "content": "<p>Hmm, let me try to reproduce it. What are the exact steps?</p>",
        "id": 420477394,
        "sender_full_name": "Kaiyu Yang",
        "timestamp": 1707404795
    },
    {
        "content": "<p>I doubt you will be able to reproduce it.  I think it is system dependent. (It doesn't happen on my laptop for example.)  I don't have a MWE yet, but it would look like this:</p>\n<ul>\n<li>Make a new lean project.</li>\n<li>Add LeanCopilot to it.</li>\n<li>Do all the lake stuff to download the models:</li>\n</ul>\n<div class=\"codehilite\" data-code-language=\"Bash\"><pre><span></span><code>lake<span class=\"w\"> </span>update<span class=\"w\"> </span>LeanCopilot\nlake<span class=\"w\"> </span>exe<span class=\"w\"> </span>LeanCopilot/download\nlake<span class=\"w\"> </span>build\n</code></pre></div>\n<ul>\n<li>Modify a lean file already being built to run <code>suggest_tactics</code>, e.g.</li>\n</ul>\n<div class=\"codehilite\" data-code-language=\"Lean\"><pre><span></span><code><span class=\"kn\">import</span> <span class=\"n\">LeanCopilot</span>\n\n<span class=\"kd\">theorem</span> <span class=\"n\">foo</span> <span class=\"o\">:</span> <span class=\"n\">true</span> <span class=\"o\">:=</span> <span class=\"kd\">by</span> <span class=\"n\">suggest_tactics</span>\n</code></pre></div>\n<ul>\n<li><code>lake build</code></li>\n<li>Get error.</li>\n</ul>\n<p>The only difference is that I'm using an existing project instead of a new one.</p>",
        "id": 420479543,
        "sender_full_name": "Jason Rute",
        "timestamp": 1707405403
    },
    {
        "content": "<p>Ok, I just followed those steps with a fresh project created by <code>lake new test_project</code>.  I get the same error.</p>",
        "id": 420481156,
        "sender_full_name": "Jason Rute",
        "timestamp": 1707405873
    },
    {
        "content": "<p>I think what is happening is that there is an error running the model.  Is there a way to run the model directly to get a less opaque error message?</p>",
        "id": 420481374,
        "sender_full_name": "Jason Rute",
        "timestamp": 1707405932
    },
    {
        "content": "<p>Maybe this python script?: <a href=\"https://github.com/lean-dojo/LeanCopilot/tree/main/python\">https://github.com/lean-dojo/LeanCopilot/tree/main/python</a></p>",
        "id": 420481908,
        "sender_full_name": "Jason Rute",
        "timestamp": 1707406076
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"115715\">@Jason Rute</span> I tried but was unable to reproduce the error. </p>\n<p>Yes, the error probably comes from the FFI part and can be avoided by running the model as an external model following these steps.  First, create an external model and configure <code>suggest_tactics</code> to use it. The code may look like this:</p>\n<div class=\"codehilite\" data-code-language=\"Lean\"><pre><span></span><code><span class=\"kn\">import</span> <span class=\"n\">LeanCopilot</span>\n\n<span class=\"kn\">open</span> <span class=\"n\">LeanCopilot</span>\n\n<span class=\"kd\">def</span> <span class=\"n\">myModel</span> <span class=\"o\">:</span> <span class=\"n\">ExternalGenerator</span> <span class=\"o\">:=</span> <span class=\"o\">{</span>\n  <span class=\"n\">name</span> <span class=\"o\">:=</span> <span class=\"s2\">\"kaiyuy/leandojo-lean4-tacgen-byt5-small\"</span>\n  <span class=\"n\">host</span> <span class=\"o\">:=</span> <span class=\"s2\">\"localhost\"</span>\n  <span class=\"n\">port</span> <span class=\"o\">:=</span> <span class=\"mi\">23337</span>\n<span class=\"o\">}</span>\n\n<span class=\"k\">#eval</span> <span class=\"n\">registerGenerator</span> <span class=\"s2\">\"kaiyuy/leandojo-lean4-tacgen-byt5-small\"</span> <span class=\"o\">(</span><span class=\"bp\">.</span><span class=\"n\">external</span> <span class=\"n\">myModel</span><span class=\"o\">)</span>\n\n<span class=\"kd\">set_option</span> <span class=\"n\">LeanCopilot.suggest_tactics.model</span> <span class=\"s2\">\"kaiyuy/leandojo-lean4-tacgen-byt5-small\"</span>\n\n<span class=\"kd\">theorem</span> <span class=\"n\">foo</span> <span class=\"o\">:</span> <span class=\"n\">true</span> <span class=\"o\">:=</span> <span class=\"kd\">by</span> <span class=\"n\">suggest_tactics</span>\n</code></pre></div>\n<p>Second, launch the server that hosts the external model. You can use the Python script here: <a href=\"https://github.com/lean-dojo/LeanCopilot/tree/main/python\">https://github.com/lean-dojo/LeanCopilot/tree/main/python</a>. And you can comment our all models in <a href=\"https://github.com/lean-dojo/LeanCopilot/blob/main/python/server.py\">this file</a> except the model you're actually going to use (<code>\"kaiyuy/leandojo-lean4-tacgen-byt5-small\"</code>?). If successful, you will see something like below.<br>\n<a href=\"/user_uploads/3121/4Wy1jwOUBYUkncVVz9xCyWIs/image.png\">image.png</a></p>\n<div class=\"message_inline_image\"><a href=\"/user_uploads/3121/4Wy1jwOUBYUkncVVz9xCyWIs/image.png\" title=\"image.png\"><img src=\"/user_uploads/3121/4Wy1jwOUBYUkncVVz9xCyWIs/image.png\"></a></div><p>Third, <code>lake build</code></p>",
        "id": 420497495,
        "sender_full_name": "Kaiyu Yang",
        "timestamp": 1707410424
    },
    {
        "content": "<p>Thanks.  I'll give this a try, but first for some reason I am getting <code>ld.lld: error: undefined symbol</code>errors even just with <code>import LeanCopilot</code> even on a fresh new project, so I have to figure that out first.</p>",
        "id": 420503449,
        "sender_full_name": "Jason Rute",
        "timestamp": 1707412069
    },
    {
        "content": "<p>(This wasn't happening early today.)</p>",
        "id": 420503473,
        "sender_full_name": "Jason Rute",
        "timestamp": 1707412079
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"115715\">Jason Rute</span> <a href=\"#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/LeanCopilot/near/420503449\">said</a>:</p>\n<blockquote>\n<p>Thanks.  I'll give this a try, but first for some reason I am getting <code>ld.lld: error: undefined symbol</code>errors even just with <code>import LeanCopilot</code> even on a fresh new project, so I have to figure that out first.</p>\n</blockquote>\n<p>No, well, the undefined symbol part is expected <em>on a fresh project</em>.<br>\nThis is because you have to link against CTranslate2.</p>\n<p>Once you're able to reproduce, I'd suggest setting up to get a core dump on the crash.<br>\nWhat sort of debugging access do you have on the cluster?<br>\nIf you manage to set up a small reproducing example, and it's on the C/C++ side of the FFI boundary to Lean, I'd like to debug it.</p>",
        "id": 420574000,
        "sender_full_name": "namibj",
        "timestamp": 1707443159
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"584504\">@Kaiyu Yang</span> there is a missing <code>withContext</code> in the LeanCopilot frontend. In version 1.1.1 try:</p>\n<div class=\"codehilite\" data-code-language=\"Lean\"><pre><span></span><code><span class=\"kn\">import</span> <span class=\"n\">LeanCopilot</span>\n<span class=\"kn\">import</span> <span class=\"n\">Mathlib.Topology.Instances.Real</span>\n\n<span class=\"kd\">def</span> <span class=\"n\">continuous_function_at</span> <span class=\"o\">(</span><span class=\"n\">f</span> <span class=\"o\">:</span> <span class=\"n\">ℝ</span> <span class=\"bp\">→</span> <span class=\"n\">ℝ</span><span class=\"o\">)</span> <span class=\"o\">(</span><span class=\"n\">x₀</span> <span class=\"o\">:</span> <span class=\"n\">ℝ</span><span class=\"o\">)</span> <span class=\"o\">:=</span>\n<span class=\"bp\">∀</span> <span class=\"n\">ε</span> <span class=\"bp\">&gt;</span> <span class=\"mi\">0</span><span class=\"o\">,</span> <span class=\"bp\">∃</span> <span class=\"n\">δ</span> <span class=\"bp\">&gt;</span> <span class=\"mi\">0</span><span class=\"o\">,</span> <span class=\"bp\">∀</span> <span class=\"n\">x</span><span class=\"o\">,</span> <span class=\"bp\">|</span><span class=\"n\">x</span> <span class=\"bp\">-</span> <span class=\"n\">x₀</span><span class=\"bp\">|</span> <span class=\"bp\">≤</span> <span class=\"n\">δ</span> <span class=\"bp\">→</span> <span class=\"bp\">|</span><span class=\"n\">f</span> <span class=\"n\">x</span> <span class=\"bp\">-</span> <span class=\"n\">f</span> <span class=\"n\">x₀</span><span class=\"bp\">|</span> <span class=\"bp\">≤</span> <span class=\"n\">ε</span>\n\n<span class=\"kd\">def</span> <span class=\"n\">sequence_tendsto</span> <span class=\"o\">(</span><span class=\"n\">u</span> <span class=\"o\">:</span> <span class=\"n\">ℕ</span> <span class=\"bp\">→</span> <span class=\"n\">ℝ</span><span class=\"o\">)</span> <span class=\"o\">(</span><span class=\"n\">l</span> <span class=\"o\">:</span> <span class=\"n\">ℝ</span><span class=\"o\">)</span> <span class=\"o\">:=</span>\n<span class=\"bp\">∀</span> <span class=\"n\">ε</span> <span class=\"bp\">&gt;</span> <span class=\"mi\">0</span><span class=\"o\">,</span> <span class=\"bp\">∃</span> <span class=\"n\">N</span><span class=\"o\">,</span> <span class=\"bp\">∀</span> <span class=\"n\">n</span> <span class=\"bp\">≥</span> <span class=\"n\">N</span><span class=\"o\">,</span> <span class=\"bp\">|</span><span class=\"n\">u</span> <span class=\"n\">n</span> <span class=\"bp\">-</span> <span class=\"n\">l</span><span class=\"bp\">|</span> <span class=\"bp\">≤</span> <span class=\"n\">ε</span>\n\n<span class=\"kd\">example</span> <span class=\"o\">(</span><span class=\"n\">f</span> <span class=\"o\">:</span> <span class=\"n\">ℝ</span> <span class=\"bp\">→</span> <span class=\"n\">ℝ</span><span class=\"o\">)</span> <span class=\"o\">(</span><span class=\"n\">u</span> <span class=\"o\">:</span> <span class=\"n\">ℕ</span> <span class=\"bp\">→</span> <span class=\"n\">ℝ</span><span class=\"o\">)</span> <span class=\"o\">(</span><span class=\"n\">x₀</span> <span class=\"o\">:</span> <span class=\"n\">ℝ</span><span class=\"o\">)</span>\n    <span class=\"o\">(</span><span class=\"n\">hu</span> <span class=\"o\">:</span> <span class=\"n\">sequence_tendsto</span> <span class=\"n\">u</span> <span class=\"n\">x₀</span><span class=\"o\">)</span> <span class=\"o\">(</span><span class=\"n\">hf</span> <span class=\"o\">:</span> <span class=\"n\">continuous_function_at</span> <span class=\"n\">f</span> <span class=\"n\">x₀</span><span class=\"o\">)</span> <span class=\"o\">:</span>\n    <span class=\"n\">sequence_tendsto</span> <span class=\"o\">(</span><span class=\"n\">f</span> <span class=\"bp\">∘</span> <span class=\"n\">u</span><span class=\"o\">)</span> <span class=\"o\">(</span><span class=\"n\">f</span> <span class=\"n\">x₀</span><span class=\"o\">)</span> <span class=\"o\">:=</span> <span class=\"kd\">by</span>\n  <span class=\"n\">unfold</span>  <span class=\"n\">sequence_tendsto</span>\n  <span class=\"n\">suggest_tactics</span>\n  <span class=\"gr\">sorry</span>\n</code></pre></div>\n<p>and all those <code>_uniq</code> in the remaining goals:</p>\n<div class=\"codehilite\" data-code-language=\"Lean\"><pre><span></span><code><span class=\"n\">Try</span> <span class=\"n\">these</span><span class=\"o\">:</span>\n<span class=\"n\">refine'</span> <span class=\"k\">fun</span> <span class=\"n\">ε</span> <span class=\"n\">ε_pos</span> <span class=\"bp\">=&gt;</span>  <span class=\"n\">_</span>\n<span class=\"n\">Remaining</span> <span class=\"n\">subgoals</span><span class=\"o\">:</span>\n<span class=\"bp\">⊢</span> <span class=\"bp\">∃</span> <span class=\"n\">N</span><span class=\"o\">,</span> <span class=\"bp\">∀</span> <span class=\"n\">n</span> <span class=\"bp\">≥</span> <span class=\"n\">N</span><span class=\"o\">,</span> <span class=\"bp\">|</span><span class=\"o\">(</span><span class=\"n\">f</span> <span class=\"bp\">∘</span> <span class=\"n\">u</span><span class=\"o\">)</span> <span class=\"n\">n</span> <span class=\"bp\">-</span> <span class=\"n\">f</span> <span class=\"n\">x₀</span><span class=\"bp\">|</span> <span class=\"bp\">≤</span> <span class=\"n\">_uniq.19267</span>\n<span class=\"n\">intro</span> <span class=\"n\">ε</span>  <span class=\"n\">hε</span>\n<span class=\"n\">Remaining</span> <span class=\"n\">subgoals</span><span class=\"o\">:</span>\n<span class=\"bp\">⊢</span> <span class=\"bp\">∃</span> <span class=\"n\">N</span><span class=\"o\">,</span> <span class=\"bp\">∀</span> <span class=\"n\">n</span> <span class=\"bp\">≥</span> <span class=\"n\">N</span><span class=\"o\">,</span> <span class=\"bp\">|</span><span class=\"o\">(</span><span class=\"n\">f</span> <span class=\"bp\">∘</span> <span class=\"n\">u</span><span class=\"o\">)</span> <span class=\"n\">n</span> <span class=\"bp\">-</span> <span class=\"n\">f</span> <span class=\"n\">x₀</span><span class=\"bp\">|</span> <span class=\"bp\">≤</span> <span class=\"n\">_uniq.772</span>\n<span class=\"n\">intro</span>  <span class=\"n\">ε</span>\n<span class=\"n\">Remaining</span> <span class=\"n\">subgoals</span><span class=\"o\">:</span>\n<span class=\"bp\">⊢</span> <span class=\"n\">_uniq.782</span> <span class=\"bp\">&gt;</span> <span class=\"mi\">0</span> <span class=\"bp\">→</span> <span class=\"bp\">∃</span> <span class=\"n\">N</span><span class=\"o\">,</span> <span class=\"bp\">∀</span> <span class=\"n\">n</span> <span class=\"bp\">≥</span> <span class=\"n\">N</span><span class=\"o\">,</span> <span class=\"bp\">|</span><span class=\"o\">(</span><span class=\"n\">f</span> <span class=\"bp\">∘</span> <span class=\"n\">u</span><span class=\"o\">)</span> <span class=\"n\">n</span> <span class=\"bp\">-</span> <span class=\"n\">f</span> <span class=\"n\">x₀</span><span class=\"bp\">|</span> <span class=\"bp\">≤</span> <span class=\"n\">_uniq.782</span>\n<span class=\"n\">intro</span> <span class=\"n\">ε</span>  <span class=\"n\">εpos</span>\n<span class=\"n\">Remaining</span> <span class=\"n\">subgoals</span><span class=\"o\">:</span>\n<span class=\"bp\">⊢</span> <span class=\"bp\">∃</span> <span class=\"n\">N</span><span class=\"o\">,</span> <span class=\"bp\">∀</span> <span class=\"n\">n</span> <span class=\"bp\">≥</span> <span class=\"n\">N</span><span class=\"o\">,</span> <span class=\"bp\">|</span><span class=\"o\">(</span><span class=\"n\">f</span> <span class=\"bp\">∘</span> <span class=\"n\">u</span><span class=\"o\">)</span> <span class=\"n\">n</span> <span class=\"bp\">-</span> <span class=\"n\">f</span> <span class=\"n\">x₀</span><span class=\"bp\">|</span> <span class=\"bp\">≤</span> <span class=\"n\">_uniq.790</span>\n</code></pre></div>",
        "id": 422514639,
        "sender_full_name": "Patrick Massot",
        "timestamp": 1708469513
    },
    {
        "content": "<p>The fix is very easy. In the Frontend file, replace</p>\n<div class=\"codehilite\" data-code-language=\"Lean\"><pre><span></span><code>    <span class=\"k\">let</span> <span class=\"n\">mut</span> <span class=\"n\">str</span> <span class=\"o\">:=</span> <span class=\"s2\">\"</span><span class=\"se\">\\n</span><span class=\"s2\">Remaining subgoals:\"</span>\n    <span class=\"n\">for</span> <span class=\"n\">g</span> <span class=\"k\">in</span> <span class=\"n\">goals</span> <span class=\"k\">do</span>\n      <span class=\"k\">let</span> <span class=\"n\">e</span> <span class=\"bp\">←</span> <span class=\"n\">PrettyPrinter.ppExpr</span> <span class=\"o\">(</span><span class=\"bp\">←</span> <span class=\"n\">instantiateMVars</span> <span class=\"o\">(</span><span class=\"bp\">←</span> <span class=\"n\">g.getType</span><span class=\"o\">))</span>\n      <span class=\"n\">str</span> <span class=\"o\">:=</span> <span class=\"n\">str</span> <span class=\"bp\">++</span> <span class=\"n\">Format.pretty</span> <span class=\"o\">(</span><span class=\"s2\">\"</span><span class=\"se\">\\n</span><span class=\"s2\">⊢ \"</span> <span class=\"bp\">++</span> <span class=\"n\">e</span><span class=\"o\">)</span>\n</code></pre></div>\n<p>with</p>\n<div class=\"codehilite\" data-code-language=\"Lean\"><pre><span></span><code>    <span class=\"k\">let</span> <span class=\"n\">mut</span> <span class=\"n\">str</span> <span class=\"o\">:=</span> <span class=\"s2\">\"</span><span class=\"se\">\\n</span><span class=\"s2\">Remaining subgoals:\"</span>\n    <span class=\"n\">for</span> <span class=\"n\">g</span> <span class=\"k\">in</span> <span class=\"n\">goals</span> <span class=\"k\">do</span>\n      <span class=\"k\">let</span> <span class=\"n\">goalType</span> <span class=\"bp\">←</span> <span class=\"n\">instantiateMVars</span> <span class=\"o\">(</span><span class=\"bp\">←</span> <span class=\"n\">g.getType</span><span class=\"o\">)</span>\n      <span class=\"k\">let</span> <span class=\"n\">e</span> <span class=\"bp\">←</span> <span class=\"n\">g.withContext</span> <span class=\"k\">do</span> <span class=\"o\">(</span><span class=\"n\">PrettyPrinter.ppExpr</span> <span class=\"n\">goalType</span><span class=\"o\">)</span>\n      <span class=\"n\">str</span> <span class=\"o\">:=</span> <span class=\"n\">str</span> <span class=\"bp\">++</span> <span class=\"n\">Format.pretty</span> <span class=\"o\">(</span><span class=\"s2\">\"</span><span class=\"se\">\\n</span><span class=\"s2\">⊢ \"</span> <span class=\"bp\">++</span> <span class=\"n\">e</span><span class=\"o\">)</span>\n</code></pre></div>",
        "id": 422514784,
        "sender_full_name": "Patrick Massot",
        "timestamp": 1708469585
    },
    {
        "content": "<p>I am sorry I don’t have much time for a proper bug report+PR right now.</p>",
        "id": 422514865,
        "sender_full_name": "Patrick Massot",
        "timestamp": 1708469634
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"110031\">@Patrick Massot</span> Thanks! I'll take a look once I have a chance!</p>",
        "id": 422660797,
        "sender_full_name": "Kaiyu Yang",
        "timestamp": 1708533657
    },
    {
        "content": "<p>Hi all, I managed to install lean copilot with a lot of effort on windows wsl, apparently there are no errors when importing leanCopilot in the.lean file. The problem arises when I execute a command like \"suggest_tactics\", after a few seconds I get the following error. Maybe someone has already experienced this or knows how to fix this error?</p>\n<div class=\"codehilite\" data-code-language=\"Lean\"><pre><span></span><code><span class=\"kn\">import</span> <span class=\"n\">LeanCopilot</span>\n\n<span class=\"kd\">theorem</span> <span class=\"n\">hello_world</span> <span class=\"o\">(</span><span class=\"n\">a</span> <span class=\"n\">b</span> <span class=\"n\">c</span> <span class=\"o\">:</span> <span class=\"n\">Nat</span><span class=\"o\">)</span>\n  <span class=\"o\">:</span> <span class=\"n\">a</span> <span class=\"bp\">+</span> <span class=\"n\">b</span> <span class=\"bp\">+</span> <span class=\"n\">c</span> <span class=\"bp\">=</span> <span class=\"n\">a</span> <span class=\"bp\">+</span> <span class=\"n\">c</span> <span class=\"bp\">+</span> <span class=\"n\">b</span> <span class=\"o\">:=</span> <span class=\"kd\">by</span>\n  <span class=\"n\">suggest_tactics</span>\n</code></pre></div>\n<p><a href=\"/user_uploads/3121/aDnoBAStl4oHz8A9geSOsSVP/imagen.png\">imagen.png</a></p>\n<div class=\"message_inline_image\"><a href=\"/user_uploads/3121/aDnoBAStl4oHz8A9geSOsSVP/imagen.png\" title=\"imagen.png\"><img src=\"/user_uploads/3121/aDnoBAStl4oHz8A9geSOsSVP/imagen.png\"></a></div>",
        "id": 430632168,
        "sender_full_name": "Esteban Estupinan",
        "timestamp": 1711991911
    },
    {
        "content": "<p>I've been trying LeanCopilot's <code>search_proof</code> on <a href=\"https://github.com/dwrensha/compfiles\">Compfiles</a> via <a href=\"https://github.com/dwrensha/tryAtEachStep\">tryAtEachStep</a>. Last night it found <a href=\"https://github.com/dwrensha/compfiles/commit/230d8fe0af1aa0ae153e76a8e4831228519304d9\">this nice simplification</a>, shortening eleven lines to three lines. </p>\n<p>(The literal thing it suggested was</p>\n<div class=\"codehilite\" data-code-language=\"Lean\"><pre><span></span><code>  <span class=\"n\">apply</span> <span class=\"n\">mul_nonneg</span>\n  <span class=\"bp\">·</span> <span class=\"n\">simp_all</span> <span class=\"n\">only</span> <span class=\"o\">[</span><span class=\"n\">sub_nonneg</span><span class=\"o\">]</span>\n  <span class=\"bp\">·</span> <span class=\"n\">simp_all</span> <span class=\"n\">only</span> <span class=\"o\">[</span><span class=\"n\">sub_nonneg</span><span class=\"o\">]</span>\n    <span class=\"n\">nlinarith</span>\n</code></pre></div>\n<p>and I touched that up a bit before committing.)</p>",
        "id": 433744703,
        "sender_full_name": "David Renshaw",
        "timestamp": 1713357170
    },
    {
        "content": "<p>Lean Copilot's preprint is out: <a href=\"https://arxiv.org/abs/2404.12534\">https://arxiv.org/abs/2404.12534</a></p>",
        "id": 434752009,
        "sender_full_name": "Kaiyu Yang",
        "timestamp": 1713793075
    },
    {
        "content": "<p>Nice.  I see you provide a benchmark, always appreciated.  Do you have the benchmarking code or at least a list of test theorems so that others can compare their models on the same theorems?</p>",
        "id": 434754887,
        "sender_full_name": "Jason Rute",
        "timestamp": 1713793761
    },
    {
        "content": "<p>Also what were the specs and time limits for the automatic proofs?</p>",
        "id": 434758145,
        "sender_full_name": "Jason Rute",
        "timestamp": 1713794461
    },
    {
        "content": "<p>Hi <span class=\"user-mention\" data-user-id=\"115715\">@Jason Rute</span> , thanks! We detail the list of test theorems and their respective results in the Appendix (page 6) of our <a href=\"https://mathai2023.github.io/papers/4.pdf\">previous short paper</a>. We will add the information to our Arxiv version too in its next edition. For all the experiments, we sticked to the default configuration (e.g. maxHeartBeats) in <code>aesop</code> without any customized configurations.</p>",
        "id": 434826623,
        "sender_full_name": "Peiyang Song",
        "timestamp": 1713813098
    }
]