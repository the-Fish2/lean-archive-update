[
    {
        "content": "<p>Hi all, </p>\n<p><span class=\"user-mention\" data-user-id=\"584504\">@Kaiyu Yang</span>  and I are excited to share our paper <em>Autoformalizing Euclidean Geometry</em> which will appear at ICML 2024. The <a href=\"https://arxiv.org/abs/2405.17216\">paper</a> and <a href=\"https://github.com/loganrjmurphy/LeanEuclid\">code</a> are publicly available.</p>\n<p><strong>Summary</strong></p>\n<p>We extend Lean with an SMT-based reasoning engine to tackle two challenges faced when autoformalizing Euclidean geometry:</p>\n<ol>\n<li>handling the abundance of implicit diagrammatic reasoning encountered in natural language proofs, and</li>\n<li>comparing the semantics of autoformalized propositions against ground truth formalizations.</li>\n</ol>\n<p>We provide a dataset of 173 theorems and proofs in natural language, Lean, and diagrams, including the first book of Euclid's <em>Elements</em>. Based on experiments with GPT-4 and GPT-4 Vision, we observe that</p>\n<ol>\n<li>Autoformalized proofs targeting our language, while rarely <em>exactly correct</em>, are often quite reasonable and easy to repair</li>\n<li>While our symbolic reasoning engine makes autoformalization in Euclidean geometry a rigorous and controllable domain, our specification language is a challenging target for formalizing theorem statements.</li>\n</ol>\n<p>We hope that our language and benchmark will be useful for researchers interested in AI-supported geometry, and that our overall approach may serve as a proof-of-concept for leveraging symbolic techniques in other autoformalization domains. </p>\n<p>In addition to Kaiyu and myself, this work is a collaboration with Jack Sun (UofT), Zhaoyu Li (UofT), Anima Anandkumar (Caltech) and Xujie Si (UofT).</p>",
        "id": 441082099,
        "sender_full_name": "Logan Murphy",
        "timestamp": 1716916527
    },
    {
        "content": "<p>Here are some more details, if you want an overview before diving into the paper:</p>\n<p><strong>Proof Automation for Euclidean Geometry in Lean</strong></p>\n<p>At the heart of our work is an implementation of <span class=\"user-mention\" data-user-id=\"110865\">@Jeremy Avigad</span> 's <a href=\"https://www.andrew.cmu.edu/user/avigad/Papers/formal_system_for_euclids_elements.pdf\">formal system</a> <em>E,</em> a theory for Euclidean geometry which faithfully models Euclid's particular style of reasoning in <em>Elements.</em> In our implementation, we distinguish <em>explicit</em> reasoning steps (e.g., construction of geometric objects) from <em>implicit</em> reasoning steps (i.e., preconditions for constructions/inferences which are often intuited from diagrams). We design a primarily declarative proof language in which the explicit reasoning steps are written as tactics, and implicit reasoning steps are discharged to SMT solvers. The result is that our formalizations of Euclid's proofs are very similar to the natural language proofs, avoiding much of the overhead typically encountered when formalizing geometric proofs. </p>\n<p>In the context of autoformalization, this allows the model to focus only on translating the steps which are actually present in the text.</p>\n<p><strong>Semantic Evaluation of Autoformalized Theorem Statements</strong></p>\n<p>Syntactic metrics such as BLEU are commonly used to evaluate autoformalized theorem statements, but syntactically similar theorem statements can obviously have very different meanings. As a result, syntactic metrics are usually accompanied by manual semantic evaluation, which is expensive, time-consuming and error-prone. Moreover, without an automated evaluation procedure, it is impossible to incorporate semantic evaluation in any kind of model fine-tuning. To address this challenge, we repurposed the symbolic reasoning engine underlying our proof automation to analyze the semantics of autoformalized theorem statements. Our Lean-implemented tool <code>E3</code> (Euclidean Equivalence Engine) performs standard equivalence checking (i.e., proving an <code>iff</code>) as well as a more granular approximate equivalence analysis which attempts to quantify how \"close\" an incorrect prediction is to a ground truth formalization.</p>\n<p><strong>Experiments with GPT-4 and GPT-4 Vision</strong></p>\n<p>To demonstrate how our benchmark+framework can assist in testing autoformalization models, and to see if our problems are challenging, we conducted several experiments with GPT-4 and GPT-4 Vision. </p>\n<p>When it comes to autoformalizing theorem statements, we found that our automated evaluation procedure correlates closely with manual evaluation. On the other hand, since our language is pretty low-level, actually autoformalizing statements correctly is challenging for the off-the-shelf models we evaluated.  We found only a marginal improvement of GPT-4 Vision vs GPT-4.</p>\n<p>When it comes to autoformalizing proofs, we did not do any sort of iterative/search-based formalization or theorem proving. With the goal of establishing a baseline for proof autoformalization, we just took a single query and analyzed the resulting proof. As one would expect, very few proofs were correct out-of-the-box. But encouragingly, many of the proofs we formalized were very close to correct, requiring only a small number of modifications to repair them (e.g., rearranging the arguments to a lemma). We did this by hand, but we predict that this repair process should be quite amenable to automation. Some examples of autoformalized and repaired proofs are given in the Appendix of the paper.</p>\n<p><strong>Other Notes</strong></p>\n<ol>\n<li>We did not do any kind of model development or fine-tuning aside from few-shot learning. Part of our goal was to see how much we could do by just focusing on the symbolic side of the problem.</li>\n<li>It is natural to ask what the relation of this work is to AlphaGeometry. We note that AlphaGeometry is tackling the theorem proving problem, while we focus on autoformalization.  While the problems are of course related, our work focuses on translating authentic pieces of natural language geometry into \"faithful\" formal theorems and proofs, whereas AlphaGeometry uses synthetic data for training. We view the two works as being complementary rather than one subsuming the other.</li>\n<li>Ours is not the only formalization of System E in Lean, c.f. <a href=\"https://github.com/leanprover-community/mathlib4/pull/7300\">this PR</a>  by <span class=\"user-mention\" data-user-id=\"407577\">@André Hernández-Espiet (Rutgers)</span> . But to the best of our knowledge, ours is the first to systematically offload implicit and diagrammatic reasoning to proof automation. EDIT: And, as Zhangir pointed out and I should have made more clear, our proofs are <strong>not</strong> totally certified, we treat SMT solvers as a trusted black box. </li>\n</ol>\n<p>Finally: if you find this work interesting, please consider sharing Kaiyu's <a href=\"https://x.com/KaiyuYang4/status/1795501816802603045\">announcement</a>.</p>",
        "id": 441082223,
        "sender_full_name": "Logan Murphy",
        "timestamp": 1716916569
    },
    {
        "content": "<p>Really impressive work! One thing I think it's worth pointing out is that the SMT reasoning engine introduces an additional axiom to admit goals that are discharged to the solver. <a href=\"https://github.com/loganrjmurphy/LeanEuclid/blob/master/SystemE/Meta/Smt/Solver.lean#L44-L47\">https://github.com/loganrjmurphy/LeanEuclid/blob/master/SystemE/Meta/Smt/Solver.lean#L44-L47</a></p>\n<div class=\"codehilite\" data-code-language=\"Lean4\"><pre><span></span><code><span class=\"c\">/-</span><span class=\"cm\"> Unsound axiom we use to admit results from the solvers</span>\n<span class=\"cm\">   Dangerous!</span>\n<span class=\"cm\"> -/</span>\n<span class=\"kn\">axiom</span><span class=\"w\"> </span><span class=\"n\">SMT_VERIF</span><span class=\"w\"> </span><span class=\"o\">(</span><span class=\"n\">P</span><span class=\"w\"> </span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"kt\">Prop</span><span class=\"o\">)</span><span class=\"w\"> </span><span class=\"o\">:</span><span class=\"w\"> </span><span class=\"n\">P</span>\n</code></pre></div>\n<p>Therefore, the comparison to Hernandez-Espiet (2023) and Beeson et al. (2019) is not quite fair, because those formalizations are end-to-end verified without introducing unsound axioms.</p>",
        "id": 441114122,
        "sender_full_name": "Zhangir Azerbayev",
        "timestamp": 1716929917
    },
    {
        "content": "<p>Yes, this is a really good point Zhangir, thank you! Our proofs are not end-to-end certified. We do make note of this in body of the paper but we will make another note in the appendix where we compare our formalization to others' (and in the repo).</p>",
        "id": 441114562,
        "sender_full_name": "Logan Murphy",
        "timestamp": 1716930164
    }
]